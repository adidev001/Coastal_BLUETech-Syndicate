{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP6sbGMkWbksCHYUAcWkB++",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nuckleheadninja/Coastal_BLUETech-Syndicate/blob/main/hackathon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPcU4SI8tPHu"
      },
      "outputs": [],
      "source": [
        "#this is for vit vs imaagenet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"surajit651/souvikdataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCX6c5PzvuiF",
        "outputId": "5ad4c66a-bac0-405f-b7ed-73a49b648f2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.2).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/surajit651/souvikdataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.58G/1.58G [00:16<00:00, 105MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/surajit651/souvikdataset/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"adidev001/procesed-again-costal-polutant\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hlte4NnOySWw",
        "outputId": "ea689d92-4fd4-4346-f51d-1724ef279c0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.13), please consider upgrading to the latest version (0.4.2).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/adidev001/procesed-again-costal-polutant?dataset_version_number=2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 338M/338M [00:02<00:00, 168MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/adidev001/procesed-again-costal-polutant/versions/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmrFO5_DTtv4",
        "outputId": "00ec31d7-86c7-4434-98c9-9c935d27c66f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasetPath = \"/root/.cache/kagglehub/datasets/adidev001/procesed-again-costal-polutant/versions/2/balanced_dataset\"\n"
      ],
      "metadata": {
        "id": "8oDvlNDBUl3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "\n",
        "dataset = datasets.ImageFolder(datasetPath)\n",
        "print(dataset.classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOf47Y0uVxin",
        "outputId": "7e4775ad-84c8-4152-fbbf-dfc580145310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cardboard', 'clean_water', 'glass', 'marine_trash', 'metal', 'oil_spill', 'paper', 'plastic']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import timm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HRAj7gpUu4n",
        "outputId": "23c81697-728b-4a18-a7a1-ccdefff7ccc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.24)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.24.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (1.3.7)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (26.0)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.5.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.67.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (11.3.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub->timm) (0.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface_hub->timm) (8.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareTransforms():\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "894vLq63U8x-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadDataset(path):\n",
        "    transform = prepareTransforms()\n",
        "    dataset = datasets.ImageFolder(path, transform=transform)\n",
        "    return dataset\n",
        "dataset = loadDataset(datasetPath)\n",
        "print(\"Total images:\", len(dataset))\n",
        "print(\"Classes:\", dataset.classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SL7oLW1VCQ3",
        "outputId": "65b503fa-3b13-4f52-84d3-47ade335b198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 14493\n",
            "Classes: ['cardboard', 'clean_water', 'glass', 'marine_trash', 'metal', 'oil_spill', 'paper', 'plastic']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "def splitDataset(dataset):\n",
        "    trainSize = int(0.7 * len(dataset))\n",
        "    valSize = len(dataset) - trainSize\n",
        "\n",
        "    trainData, valData = random_split(dataset, [trainSize, valSize])\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Training images:\", len(trainData))\n",
        "    print(\"Validation images:\", len(valData))\n",
        "\n",
        "    return trainData, valData\n",
        "trainData, valData = splitDataset(dataset)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ism8LXK_VLfn",
        "outputId": "89da86fd-f69f-4c43-9953-d7fc7ba1f080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training images: 10145\n",
            "Validation images: 4348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def createLoaders(trainData, valData):\n",
        "    trainLoader = DataLoader(trainData, batch_size=32, shuffle=True)\n",
        "    valLoader = DataLoader(valData, batch_size=32, shuffle=False)\n",
        "    return trainLoader, valLoader\n",
        "trainLoader, valLoader = createLoaders(trainData, valData)\n"
      ],
      "metadata": {
        "id": "vyUMVKG-XNtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadEfficientNet(classCount):\n",
        "    model = timm.create_model(\n",
        "        \"efficientnet_b0\",\n",
        "        pretrained=True,\n",
        "        num_classes=classCount\n",
        "    )\n",
        "    return model\n",
        "classCount = len(dataset.classes)\n",
        "model = loadEfficientNet(classCount)\n"
      ],
      "metadata": {
        "id": "2qG11ZNKXxlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def freezeModel(model):\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    for param in model.classifier.parameters():\n",
        "        param.requires_grad = True\n",
        "freezeModel(model)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "lossFunction = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "def trainModel(model, loader):\n",
        "    model.train()\n",
        "    totalLoss = 0\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = lossFunction(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        totalLoss += loss.item()\n",
        "\n",
        "    return totalLoss / len(loader)\n",
        "def validateModel(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return (correct / total) * 100\n",
        "for epoch in range(5):\n",
        "    trainLoss = trainModel(model, trainLoader)\n",
        "    valAccuracy = validateModel(model, valLoader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}\")\n",
        "    print(\"Train Loss:\", trainLoss)\n",
        "    print(\"Val Accuracy:\", valAccuracy)\n",
        "torch.save(model.state_dict(), \"efficientnetMarine.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdm8zcY-X-Aw",
        "outputId": "bd680d27-f240-44f1-8ace-8f3275e68b36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Train Loss: 1.5557196124357247\n",
            "Val Accuracy: 73.62005519779208\n",
            "Epoch 2\n",
            "Train Loss: 0.6639168580185693\n",
            "Val Accuracy: 82.15271389144434\n",
            "Epoch 3\n",
            "Train Loss: 0.5042528400443634\n",
            "Val Accuracy: 83.90064397424104\n",
            "Epoch 4\n",
            "Train Loss: 0.4395507459511172\n",
            "Val Accuracy: 86.17755289788408\n",
            "Epoch 5\n",
            "Train Loss: 0.39042659501000393\n",
            "Val Accuracy: 86.86752529898804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# 1. INSTALL & IMPORT LIBRARIES\n",
        "# ==============================\n",
        "!pip install timm tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import timm\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ==============================\n",
        "# 2. DATASET PATH (CHANGE IF NEEDED)\n",
        "# ==============================\n",
        "datasetPath = \"/root/.cache/kagglehub/datasets/adidev001/procesed-again-costal-polutant/versions/2/balanced_dataset\"\n",
        "\n",
        "# ==============================\n",
        "# 3. IMAGE TRANSFORMS\n",
        "# ==============================\n",
        "def prepareTransforms():\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225]\n",
        "        )\n",
        "    ])\n",
        "\n",
        "# ==============================\n",
        "# 4. LOAD DATASET\n",
        "# ==============================\n",
        "dataset = datasets.ImageFolder(datasetPath, transform=prepareTransforms())\n",
        "print(\"Total images:\", len(dataset))\n",
        "print(\"Classes:\", dataset.classes)\n",
        "\n",
        "# ==============================\n",
        "# 5. TRAIN / VAL SPLIT\n",
        "# ==============================\n",
        "def splitDataset(dataset):\n",
        "    trainSize = int(0.7 * len(dataset))\n",
        "    valSize = len(dataset) - trainSize\n",
        "    trainData, valData = random_split(dataset, [trainSize, valSize])\n",
        "\n",
        "    print(\"Training images:\", len(trainData))\n",
        "    print(\"Validation images:\", len(valData))\n",
        "\n",
        "    return trainData, valData\n",
        "\n",
        "trainData, valData = splitDataset(dataset)\n",
        "\n",
        "# ==============================\n",
        "# 6. DATALOADERS\n",
        "# ==============================\n",
        "trainLoader = DataLoader(trainData, batch_size=32, shuffle=True)\n",
        "valLoader = DataLoader(valData, batch_size=32, shuffle=False)\n",
        "\n",
        "# ==============================\n",
        "# 7. LOAD PRETRAINED EFFICIENTNET\n",
        "# ==============================\n",
        "classCount = len(dataset.classes)\n",
        "\n",
        "model = timm.create_model(\n",
        "    \"efficientnet_b0\",\n",
        "    pretrained=True,\n",
        "    num_classes=classCount\n",
        ")\n",
        "\n",
        "# ==============================\n",
        "# 8. UNFREEZE ENTIRE MODEL\n",
        "# ==============================\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# ==============================\n",
        "# 9. DEVICE, LOSS, OPTIMIZER\n",
        "# ==============================\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "lossFunction = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "# ==============================\n",
        "# 10. TRAIN FUNCTION (WITH PROGRESS)\n",
        "# ==============================\n",
        "def trainModel(model, loader, epochNumber):\n",
        "    model.train()\n",
        "    totalLoss = 0\n",
        "\n",
        "    progressBar = tqdm(loader, desc=f\"Training Epoch {epochNumber}\", leave=False)\n",
        "\n",
        "    for images, labels in progressBar:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = lossFunction(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        totalLoss += loss.item()\n",
        "        progressBar.set_postfix(loss=loss.item())\n",
        "\n",
        "    return totalLoss / len(loader)\n",
        "\n",
        "# ==============================\n",
        "# 11. VALIDATION FUNCTION\n",
        "# ==============================\n",
        "def validateModel(model, loader, epochNumber):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    progressBar = tqdm(loader, desc=f\"Validating Epoch {epochNumber}\", leave=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in progressBar:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return (correct / total) * 100\n",
        "\n",
        "# ==============================\n",
        "# 12. TRAIN LOOP (FULL FINE-TUNING)\n",
        "# ==============================\n",
        "epochs = 9\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "    trainLoss = trainModel(model, trainLoader, epoch+1)\n",
        "    valAccuracy = validateModel(model, valLoader, epoch+1)\n",
        "\n",
        "    print(\"Train Loss:\", trainLoss)\n",
        "    print(\"Val Accuracy:\", valAccuracy)\n",
        "\n",
        "# ==============================\n",
        "# 13. SAVE MODEL\n",
        "# ==============================\n",
        "print(\"Model saved successfully\")\n",
        "torch.save(model.state_dict(), \"efficientnetMarineBest90.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UtGGhUyaR59",
        "outputId": "b2aa3657-46d4-405e-bbf7-9fda0431d3e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.24)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.24.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (1.3.7)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (26.0)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (11.3.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub->timm) (0.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface_hub->timm) (8.3.1)\n",
            "Total images: 14493\n",
            "Classes: ['cardboard', 'clean_water', 'glass', 'marine_trash', 'metal', 'oil_spill', 'paper', 'plastic']\n",
            "Training images: 10145\n",
            "Validation images: 4348\n",
            "\n",
            "Epoch 1/9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 2.058082681969277\n",
            "Val Accuracy: 67.38730450781969\n",
            "\n",
            "Epoch 2/9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8060232837620022\n",
            "Val Accuracy: 79.18583256669733\n",
            "\n",
            "Epoch 3/9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.5070470715843657\n",
            "Val Accuracy: 83.76264949402024\n",
            "\n",
            "Epoch 4/9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.4034265555300803\n",
            "Val Accuracy: 86.68353265869365\n",
            "\n",
            "Epoch 5/9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.30667541300257045\n",
            "Val Accuracy: 88.43146274149034\n",
            "\n",
            "Epoch 6/9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.24689928263004096\n",
            "Val Accuracy: 90.04139834406624\n",
            "\n",
            "Epoch 7/9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.21499980535686203\n",
            "Val Accuracy: 90.43238270469182\n",
            "\n",
            "Epoch 8/9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.17936713235312193\n",
            "Val Accuracy: 90.91536338546457\n",
            "\n",
            "Epoch 9/9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.14317669420731519\n",
            "Val Accuracy: 92.0883164673413\n",
            "Model saved successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "import clip\n",
        "from PIL import Image\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def loadClipModel():\n",
        "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "    return model, preprocess\n",
        "\n",
        "clipModel, clipPreprocess = loadClipModel()\n",
        "['cardboard', 'clean_water', 'glass',\n",
        " 'marine_trash', 'metal', 'oil_spill',\n",
        " 'paper', 'plastic']\n",
        "clipSeverityLabels = [\n",
        "    \"large scale marine pollution with many plastic wastes and trash piles\",\n",
        "    \"heavy ocean pollution with dense garbage floating on water\",\n",
        "    \"massive oil spill covering the sea surface\",\n",
        "    \"clean ocean water with no visible pollution\",\n",
        "    \"natural beach or sea with no trash\"\n",
        "]\n",
        "HEAVY_INDICES = [0, 1, 2]     # heavy pollution\n",
        "GENERAL_SEVERITY_THRESHOLD = 0.35\n",
        "OIL_SPILL_INDEX = 2\n",
        "OIL_SEVERITY_THRESHOLD = 0.25\n",
        "def clipSeverityCheck(imagePath):\n",
        "    image = clipPreprocess(Image.open(imagePath)).unsqueeze(0).to(device)\n",
        "    text = clip.tokenize(clipSeverityLabels).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        imageFeatures = clipModel.encode_image(image)\n",
        "        textFeatures = clipModel.encode_text(text)\n",
        "        similarity = (imageFeatures @ textFeatures.T).softmax(dim=-1)\n",
        "\n",
        "    scores = similarity.squeeze().cpu().numpy()\n",
        "    bestIndex = scores.argmax()\n",
        "    confidence = scores[bestIndex]\n",
        "\n",
        "    return bestIndex, confidence\n",
        "def finalPrediction(imagePath):\n",
        "    severityIndex, severityConf = clipSeverityCheck(imagePath)\n",
        "\n",
        "    # ---- Oil spill special handling\n",
        "    if severityIndex == OIL_SPILL_INDEX and severityConf >= OIL_SEVERITY_THRESHOLD:\n",
        "        cnnScores = cnnPredict(imagePath)\n",
        "        bestIndex = cnnScores.argmax()\n",
        "        return dataset.classes[bestIndex], cnnScores[bestIndex] * 100\n",
        "\n",
        "    # ---- General rejection\n",
        "    if severityIndex not in HEAVY_INDICES or severityConf < GENERAL_SEVERITY_THRESHOLD:\n",
        "        return \"No marine pollution detected\", severityConf * 100\n",
        "\n",
        "    # ---- Heavy pollution → CNN classify\n",
        "    cnnScores = cnnPredict(imagePath)\n",
        "    bestIndex = cnnScores.argmax()\n",
        "    return dataset.classes[bestIndex], cnnScores[bestIndex] * 100\n",
        "def predictImage(imagePath):\n",
        "    print(\"Analyzing image:\", imagePath)\n",
        "\n",
        "    severityIndex, severityConf = clipSeverityCheck(imagePath)\n",
        "\n",
        "    print(\"\\nCLIP Severity Analysis\")\n",
        "    print(\"----------------------\")\n",
        "    print(\"Matched description:\", clipSeverityLabels[severityIndex])\n",
        "    print(f\"Confidence: {severityConf*100:.2f}%\")\n",
        "\n",
        "    label, confidence = finalPrediction(imagePath)\n",
        "\n",
        "    print(\"\\nFINAL RESULT\")\n",
        "    print(\"------------\")\n",
        "    print(\"Prediction:\", label)\n",
        "    print(f\"Confidence: {confidence:.2f}%\")\n",
        "testImagePath = \"/content/WhatsApp Image 2026-02-07 at 9.55.51 PM.jpeg\"\n",
        "predictImage(testImagePath)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgPU9jAXegtX",
        "outputId": "2a59917c-102f-4b3c-b016-b8623b5aa08c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-2i277u2b\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-2i277u2b\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (26.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (4.67.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (0.24.0+cu126)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->clip==1.0) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\n",
            "Analyzing image: /content/WhatsApp Image 2026-02-07 at 9.55.51 PM.jpeg\n",
            "\n",
            "CLIP Severity Analysis\n",
            "----------------------\n",
            "Matched description: clean ocean water with no visible pollution\n",
            "Confidence: 86.94%\n",
            "\n",
            "FINAL RESULT\n",
            "------------\n",
            "Prediction: No marine pollution detected\n",
            "Confidence: 86.94%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clipSeverityCheck(imagePath):\n",
        "    image = clipPreprocess(Image.open(imagePath)).unsqueeze(0).to(device)\n",
        "    text = clip.tokenize(clipSeverityLabels).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        imageFeatures = clipModel.encode_image(image)\n",
        "        textFeatures = clipModel.encode_text(text)\n",
        "\n",
        "        similarity = (imageFeatures @ textFeatures.T).softmax(dim=-1)\n",
        "\n",
        "    scores = similarity.squeeze().cpu().numpy()\n",
        "    bestIndex = scores.argmax()\n",
        "    confidence = scores[bestIndex]\n",
        "\n",
        "    return bestIndex, confidence\n",
        "HEAVY_INDICES = [0, 1, 2]  # first three labels are heavy pollution\n",
        "SEVERITY_THRESHOLD = 0.35\n",
        "def finalPrediction(imagePath):\n",
        "    severityIndex, severityConf = clipSeverityCheck(imagePath)\n",
        "\n",
        "    # --------------------------------\n",
        "    # NOT HEAVY POLLUTION → REJECT\n",
        "    # --------------------------------\n",
        "    if severityIndex not in HEAVY_INDICES or severityConf < SEVERITY_THRESHOLD:\n",
        "        return \"No marine pollution detected\", severityConf * 100\n",
        "\n",
        "    # --------------------------------\n",
        "    # HEAVY POLLUTION → CLASSIFY TYPE\n",
        "    # --------------------------------\n",
        "    cnnScores = cnnPredict(imagePath)\n",
        "    bestIndex = cnnScores.argmax()\n",
        "    confidence = cnnScores[bestIndex] * 100\n",
        "\n",
        "    return dataset.classes[bestIndex], confidence\n",
        "def predictImage(imagePath):\n",
        "    print(\"Analyzing image...\")\n",
        "    print(\"Image path:\", imagePath)\n",
        "\n",
        "    # CLIP severity check\n",
        "    severityIndex, severityConf = clipSeverityCheck(imagePath)\n",
        "\n",
        "    print(\"\\nCLIP Severity Analysis\")\n",
        "    print(\"----------------------\")\n",
        "    print(\"Matched description:\", clipSeverityLabels[severityIndex])\n",
        "    print(f\"Confidence: {severityConf * 100:.2f}%\")\n",
        "\n",
        "    # Decision gate\n",
        "    if severityIndex not in HEAVY_INDICES or severityConf < SEVERITY_THRESHOLD:\n",
        "        print(\"\\nFINAL RESULT\")\n",
        "        print(\"------------\")\n",
        "        print(\"No marine pollution detected\")\n",
        "        return\n",
        "\n",
        "    # CNN classification\n",
        "    cnnScores = cnnPredict(imagePath)\n",
        "    bestIndex = cnnScores.argmax()\n",
        "    confidence = cnnScores[bestIndex] * 100\n",
        "\n",
        "    print(\"\\nCNN Classification\")\n",
        "    print(\"------------------\")\n",
        "    print(\"Pollution type:\", dataset.classes[bestIndex])\n",
        "    print(f\"Confidence: {confidence:.2f}%\")\n",
        "\n",
        "    print(\"\\nFINAL RESULT\")\n",
        "    print(\"------------\")\n",
        "    print(f\"Heavy marine pollution detected → {dataset.classes[bestIndex]}\")\n",
        "testImagePath = \"/content/pexels-mika-photogenius-641920605-30379620.jpg\"  # change this path\n",
        "predictImage(testImagePath)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ymsy1t_rHDZ",
        "outputId": "8f54d976-a0f4-4944-9388-2a46a3c51107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing image...\n",
            "Image path: /content/pexels-mika-photogenius-641920605-30379620.jpg\n",
            "\n",
            "CLIP Severity Analysis\n",
            "----------------------\n",
            "Matched description: clean ocean water with no visible pollution\n",
            "Confidence: 47.25%\n",
            "\n",
            "FINAL RESULT\n",
            "------------\n",
            "No marine pollution detected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predictImage(imagePath):\n",
        "    print(\"Analyzing image...\")\n",
        "    print(\"Image path:\", imagePath)\n",
        "\n",
        "    # CLIP severity check\n",
        "    severityIndex, severityConf = clipSeverityCheck(imagePath)\n",
        "\n",
        "    print(\"\\nCLIP Severity Analysis\")\n",
        "    print(\"----------------------\")\n",
        "    print(\"Matched description:\", clipSeverityLabels[severityIndex])\n",
        "    print(f\"Confidence: {severityConf * 100:.2f}%\")\n",
        "\n",
        "    # Decision gate\n",
        "    if severityIndex not in HEAVY_INDICES or severityConf < SEVERITY_THRESHOLD:\n",
        "        print(\"\\nFINAL RESULT\")\n",
        "        print(\"------------\")\n",
        "        print(\"No marine pollution detected\")\n",
        "        return\n",
        "\n",
        "    # CNN classification\n",
        "    cnnScores = cnnPredict(imagePath)\n",
        "    bestIndex = cnnScores.argmax()\n",
        "    confidence = cnnScores[bestIndex] * 100\n",
        "\n",
        "    print(\"\\nCNN Classification\")\n",
        "    print(\"------------------\")\n",
        "    print(\"Pollution type:\", dataset.classes[bestIndex])\n",
        "    print(f\"Confidence: {confidence:.2f}%\")\n",
        "\n",
        "    print(\"\\nFINAL RESULT\")\n",
        "    print(\"------------\")\n",
        "    print(f\"Heavy marine pollution detected → {dataset.classes[bestIndex]}\")\n",
        "testImagePath = \"/content/oilspill.jpg\"  # change this path\n",
        "predictImage(testImagePath)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "srL6Zzxes6lx",
        "outputId": "bb45d606-e5a6-4b05-80b0-4b74af5fb104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing image...\n",
            "Image path: /content/oilspill.jpg\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'clipSeverityCheck' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1402169829.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Heavy marine pollution detected → {dataset.classes[bestIndex]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mtestImagePath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/oilspill.jpg\"\u001b[0m  \u001b[0;31m# change this path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mpredictImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestImagePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1402169829.py\u001b[0m in \u001b[0;36mpredictImage\u001b[0;34m(imagePath)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# CLIP severity check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mseverityIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseverityConf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclipSeverityCheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nCLIP Severity Analysis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'clipSeverityCheck' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clipPredict(imagePath):\n",
        "    image = clipPreprocess(Image.open(imagePath)).unsqueeze(0).to(device)\n",
        "    text = clip.tokenize(clipLabels).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        imageFeatures = clipModel.encode_image(image)\n",
        "        textFeatures = clipModel.encode_text(text)\n",
        "\n",
        "        similarities = (imageFeatures @ textFeatures.T).softmax(dim=-1)\n",
        "\n",
        "    return similarities.squeeze().cpu().numpy()\n",
        "def cnnPredict(imagePath):\n",
        "    image = Image.open(imagePath)\n",
        "    image = prepareTransforms()(image).unsqueeze(0).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(image)\n",
        "        probabilities = torch.softmax(outputs, dim=1)\n",
        "\n",
        "    return probabilities.squeeze().cpu().numpy()\n",
        "def ensemblePredict(imagePath):\n",
        "    cnnScores = cnnPredict(imagePath)\n",
        "    clipScores = clipPredict(imagePath)\n",
        "\n",
        "    finalScores = (cnnScores * 0.6) + (clipScores * 0.4)\n",
        "\n",
        "    bestIndex = finalScores.argmax()\n",
        "    confidence = finalScores[bestIndex] * 100\n",
        "\n",
        "    return dataset.classes[bestIndex], confidence\n",
        "label, confidence = ensemblePredict(\"/content/oilspill.jpg\")\n",
        "print(\"Prediction:\", label)\n",
        "print(\"Confidence:\", confidence)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4dbXTLSi0jk",
        "outputId": "8b926592-d7f5-41e2-db01-e95144ab960a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: oil_spill\n",
            "Confidence: 68.92578\n"
          ]
        }
      ]
    }
  ]
}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14720968,"sourceType":"datasetVersion","datasetId":9406018}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport csv\nimport math\nimport shutil\nimport random\nimport zipfile\nimport traceback\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom PIL import Image, ImageEnhance, ImageOps, ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\n\n# =========================================================\n# CONFIG (SMALL MODEL + ROBUST)\n# =========================================================\nCONFIG = {\n    \"seed\": 42,\n    \"img_size\": 224,\n    \"batch_size\": 32,\n\n    \"epochs_phase1\": 10,\n    \"epochs_phase2\": 30,\n\n    \"learning_rate\": 1e-3,\n    \"fine_tune_lr\": 1e-5,\n\n    \"dropout_rate\": 0.45,\n    \"label_smoothing\": 0.1,\n    \"early_stop_patience\": 10,\n\n    \"min_samples_per_class\": 100,\n    \"max_samples_per_class\": 900,\n\n    \"test_split\": 0.15,\n    \"val_split\": 0.15,\n\n    \"mixup\": True,\n    \"mixup_alpha\": 0.2,\n\n    \"use_mixed_precision\": True,\n\n    \"model_name\": \"mobilenetv2\",   # deploy-friendly\n    \"mobilenet_alpha\": 0.75,       # 1.0 full, 0.75 smaller, 0.5 tiny\n    \"head_units\": 128,\n\n    \"warmup_epochs\": 2,\n    \"num_workers\": tf.data.AUTOTUNE,\n\n    \"unknown_threshold\": 0.55,\n\n    \"export_tflite\": True,\n    \"export_int8\": True,\n    \"rep_data_samples\": 200,\n}\n\n# =========================================================\n# YOUR KAGGLE DATASET ROOT\n# =========================================================\nDATA_DIR = Path(\"/kaggle/input/procesed-again-costal-polutant\")\n\n# âœ… Your class folders in this dataset\nCLASS_SOURCES = {\n    \"cardboard\": DATA_DIR / \"cardboard\",\n    \"glass\": DATA_DIR / \"glass\",\n    \"metal\": DATA_DIR / \"metal\",\n    \"paper\": DATA_DIR / \"paper\",\n    \"plastic\": DATA_DIR / \"plastic\",\n    \"marine_trash\": DATA_DIR / \"processed_marine_waste\" / \"marine_trash\",\n    \"oil_spill\": DATA_DIR / \"processed_oilspill\" / \"oil_spill\",\n}\n\n# Negatives (NOT trained as classes in this script)\nNEGATIVE_SOURCES = {\n    \"clean_underwater\": DATA_DIR / \"processed_marine_waste\" / \"clean_underwater\",\n    \"no_oil_spill\": DATA_DIR / \"processed_oilspill\" / \"no_oil_spill\",\n}\n\n# =========================================================\n# OUTPUTS\n# =========================================================\nRUN_DIR = Path(\"/kaggle/working/run_output\")\nRUN_DIR.mkdir(parents=True, exist_ok=True)\n\nBALANCED_DIR = RUN_DIR / \"balanced_dataset\"\nSPLIT_DIR = RUN_DIR / \"splits\"\nTRAIN_DIR = SPLIT_DIR / \"train\"\nVAL_DIR = SPLIT_DIR / \"val\"\nTEST_DIR = SPLIT_DIR / \"test\"\n\nREPORTS_DIR = RUN_DIR / \"reports\"\nPLOTS_DIR = RUN_DIR / \"plots\"\nMODELS_DIR = RUN_DIR / \"models\"\nPREVIEW_DIR = RUN_DIR / \"aug_preview\"\nZIP_DIR = RUN_DIR / \"zips\"\n\nfor d in [REPORTS_DIR, PLOTS_DIR, MODELS_DIR, PREVIEW_DIR, ZIP_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n\nBEST_CKPT_PATH = MODELS_DIR / \"best_model.weights.h5\"\nBEST_H5_PATH = MODELS_DIR / \"best_model.h5\"\nSAVEDMODEL_PATH = MODELS_DIR / \"saved_model\"\nCLASS_MAP_PATH = MODELS_DIR / \"class_mapping.json\"\nHISTORY_CSV_PATH = REPORTS_DIR / \"training_history.csv\"\nBALANCE_REPORT_PATH = REPORTS_DIR / \"balancing_report.json\"\nSUMMARY_REPORT_PATH = REPORTS_DIR / \"training_summary.json\"\nTEST_ZIP_PATH = RUN_DIR / \"TEST.zip\"\n\nTFLITE_FP16_PATH = MODELS_DIR / \"model_fp16.tflite\"\nTFLITE_INT8_PATH = MODELS_DIR / \"model_int8.tflite\"\n\n# =========================================================\n# Utils\n# =========================================================\ndef set_seeds(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\ndef status(msg: str):\n    print(f\"\\nðŸŸ¦ {msg}\")\n\ndef warn(msg: str):\n    print(f\"ðŸŸ¨ {msg}\")\n\ndef err(msg: str):\n    print(f\"ðŸŸ¥ {msg}\")\n\ndef safe_mkdir(p: Path):\n    p.mkdir(parents=True, exist_ok=True)\n\ndef list_images(folder: Path) -> List[Path]:\n    exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\", \".tif\", \".tiff\"}\n    files = []\n    for fp in folder.rglob(\"*\"):\n        if fp.is_file() and fp.suffix.lower() in exts:\n            files.append(fp)\n    return files\n\ndef sha1_file(fp: Path, chunk_size=1024 * 1024) -> str:\n    import hashlib\n    h = hashlib.sha1()\n    with open(fp, \"rb\") as f:\n        while True:\n            b = f.read(chunk_size)\n            if not b:\n                break\n            h.update(b)\n    return h.hexdigest()\n\ndef save_json(path: Path, obj: dict):\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(obj, f, indent=2)\n\ndef read_image_pil(path: Path) -> Image.Image:\n    im = Image.open(path)\n    return im.convert(\"RGB\")\n\ndef resize_to_square(im: Image.Image, size: int) -> Image.Image:\n    return im.resize((size, size), resample=Image.BILINEAR)\n\ndef random_augment_pil(im: Image.Image) -> Image.Image:\n    if random.random() < 0.5:\n        im = ImageOps.mirror(im)\n    if random.random() < 0.25:\n        im = ImageOps.flip(im)\n\n    angle = random.uniform(-30, 30)\n    im = im.rotate(angle, resample=Image.BILINEAR, expand=False)\n\n    if random.random() < 0.7:\n        w, h = im.size\n        zoom = random.uniform(0.8, 1.2)\n        if zoom > 1.0:\n            nw, nh = int(w / zoom), int(h / zoom)\n            left = random.randint(0, max(0, w - nw))\n            top = random.randint(0, max(0, h - nh))\n            im = im.crop((left, top, left + nw, top + nh)).resize((w, h), Image.BILINEAR)\n\n    if random.random() < 0.7:\n        im = ImageEnhance.Brightness(im).enhance(random.uniform(0.8, 1.2))\n    if random.random() < 0.7:\n        im = ImageEnhance.Contrast(im).enhance(random.uniform(0.8, 1.2))\n\n    return im\n\ndef zip_folder(folder: Path, zip_path: Path):\n    if zip_path.exists():\n        zip_path.unlink()\n    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n        for fp in folder.rglob(\"*\"):\n            if fp.is_file():\n                zf.write(fp, arcname=str(fp.relative_to(folder)))\n\ndef enable_gpu_optimizations():\n    try:\n        gpus = tf.config.list_physical_devices(\"GPU\")\n        if gpus:\n            status(f\"GPU detected: {gpus}\")\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n        else:\n            warn(\"No GPU detected. Training will be slower.\")\n    except Exception as e:\n        warn(f\"GPU optimization setup failed: {e}\")\n\ndef maybe_enable_mixed_precision():\n    if not CONFIG[\"use_mixed_precision\"]:\n        return\n    try:\n        gpus = tf.config.list_physical_devices(\"GPU\")\n        if gpus:\n            from tensorflow.keras import mixed_precision\n            mixed_precision.set_global_policy(\"mixed_float16\")\n            status(\"Mixed precision enabled (fp16).\")\n        else:\n            warn(\"Mixed precision requested but no GPU found; skipping.\")\n    except Exception as e:\n        warn(f\"Mixed precision setup failed: {e}\")\n\n# =========================================================\n# Data Balancer\n# =========================================================\nclass DataBalancer:\n    def __init__(self, out_dir: Path, config: dict):\n        self.out_dir = out_dir\n        self.config = config\n        self.seed = config[\"seed\"]\n\n    def analyze_distribution(self) -> Dict[str, int]:\n        status(\"Analyzing class distribution from CLASS_SOURCES...\")\n        class_counts = {}\n\n        for class_name, class_path in CLASS_SOURCES.items():\n            if not class_path.exists():\n                warn(f\"Missing class folder: {class_name} -> {class_path}\")\n                class_counts[class_name] = 0\n                continue\n            class_counts[class_name] = len(list_images(class_path))\n\n        total = sum(class_counts.values())\n        print(\"Class distribution (BEFORE):\")\n        for k, v in class_counts.items():\n            print(f\"  - {k}: {v}\")\n        print(f\"Total images: {total}\")\n        return class_counts\n\n    def apply_augmentation(self, src_paths: List[Path], needed: int, class_name: str, dst_class_dir: Path):\n        safe_mkdir(dst_class_dir)\n        random.shuffle(src_paths)\n        status(f\"Augmenting '{class_name}' with {needed} new samples (offline)...\")\n\n        for i in tqdm(range(needed), desc=f\"Augmenting {class_name}\"):\n            try:\n                base = src_paths[i % len(src_paths)]\n                im = read_image_pil(base)\n                im = random_augment_pil(im)\n                im = resize_to_square(im, self.config[\"img_size\"])\n                out_path = dst_class_dir / f\"aug_{class_name}_{i:07d}.jpg\"\n                im.save(out_path, format=\"JPEG\", quality=90, optimize=True)\n            except Exception:\n                continue\n\n    def balance_classes(self) -> Dict[str, Dict[str, int]]:\n        set_seeds(self.seed)\n\n        # wipe old balanced dir\n        if self.out_dir.exists():\n            shutil.rmtree(self.out_dir, ignore_errors=True)\n        safe_mkdir(self.out_dir)\n\n        before = self.analyze_distribution()\n        counts = [v for v in before.values() if v > 0]\n        if not counts:\n            raise ValueError(\"No valid images found in any class folders.\")\n\n        min_count = min(counts)\n\n        target_count = max(self.config[\"min_samples_per_class\"], min_count)\n        target_count = min(target_count, self.config[\"max_samples_per_class\"])\n\n        status(f\"Balancing target per class: {target_count} images\")\n\n        seen_hashes = set()\n        report = {\"before\": before, \"after\": {}, \"target_count\": target_count}\n\n        for class_name in CLASS_SOURCES.keys():\n            src_class_dir = CLASS_SOURCES[class_name]\n            dst_class_dir = self.out_dir / class_name\n            safe_mkdir(dst_class_dir)\n\n            if not src_class_dir.exists():\n                warn(f\"Skipping missing class folder: {src_class_dir}\")\n                report[\"after\"][class_name] = 0\n                continue\n\n            all_imgs = list_images(src_class_dir)\n            random.shuffle(all_imgs)\n\n            # Global dedupe by hash\n            deduped = []\n            for fp in all_imgs:\n                try:\n                    h = sha1_file(fp)\n                except Exception:\n                    continue\n                if h in seen_hashes:\n                    continue\n                seen_hashes.add(h)\n                deduped.append(fp)\n\n            if len(deduped) == 0:\n                warn(f\"Class '{class_name}' has 0 valid images after dedupe.\")\n                report[\"after\"][class_name] = 0\n                continue\n\n            if len(deduped) > target_count:\n                keep = deduped[:target_count]\n                status(f\"'{class_name}': {len(deduped)} -> keeping {target_count} (random deletion)\")\n                for i, fp in enumerate(tqdm(keep, desc=f\"Copying {class_name}\")):\n                    try:\n                        im = read_image_pil(fp)\n                        im = resize_to_square(im, self.config[\"img_size\"])\n                        out_path = dst_class_dir / f\"{class_name}_{i:07d}.jpg\"\n                        im.save(out_path, format=\"JPEG\", quality=90, optimize=True)\n                    except Exception:\n                        continue\n            else:\n                status(f\"'{class_name}': {len(deduped)} -> boosting to {target_count} (offline augmentation)\")\n                for i, fp in enumerate(tqdm(deduped, desc=f\"Copying {class_name}\")):\n                    try:\n                        im = read_image_pil(fp)\n                        im = resize_to_square(im, self.config[\"img_size\"])\n                        out_path = dst_class_dir / f\"{class_name}_{i:07d}.jpg\"\n                        im.save(out_path, format=\"JPEG\", quality=90, optimize=True)\n                    except Exception:\n                        continue\n\n                current = len(list(dst_class_dir.glob(\"*.jpg\")))\n                needed = max(0, target_count - current)\n                if needed > 0:\n                    self.apply_augmentation(deduped, needed, class_name, dst_class_dir)\n\n            report[\"after\"][class_name] = len(list(dst_class_dir.glob(\"*.jpg\")))\n\n        save_json(BALANCE_REPORT_PATH, report)\n        status(f\"Balancing report saved to: {BALANCE_REPORT_PATH}\")\n\n        print(\"Class distribution (AFTER):\")\n        for k, v in report[\"after\"].items():\n            print(f\"  - {k}: {v}\")\n\n        return report\n\n# =========================================================\n# Trainer\n# =========================================================\nclass WarmupThenConstant(tf.keras.callbacks.Callback):\n    def __init__(self, base_lr: float, warmup_epochs: int):\n        super().__init__()\n        self.base_lr = float(base_lr)\n        self.warmup_epochs = int(warmup_epochs)\n\n    def on_epoch_begin(self, epoch, logs=None):\n        if epoch < self.warmup_epochs:\n            lr = self.base_lr * (epoch + 1) / self.warmup_epochs\n        else:\n            lr = self.base_lr\n\n        opt = self.model.optimizer\n        # Keras-safe LR set (handles Variable / schedule / weird wrappers)\n        try:\n            if hasattr(opt, \"learning_rate\"):\n                if hasattr(opt.learning_rate, \"assign\"):\n                    opt.learning_rate.assign(lr)\n                else:\n                    # some optimizers keep lr inside _learning_rate\n                    if hasattr(opt, \"_learning_rate\") and hasattr(opt._learning_rate, \"assign\"):\n                        opt._learning_rate.assign(lr)\n                    else:\n                        # last resort: just replace attribute\n                        opt.learning_rate = lr\n        except Exception as e:\n            print(f\"ðŸŸ¨ Warmup LR set failed, skipping. Reason: {e}\")\n\n\nclass AntiOverfitTrainer:\n    def __init__(self, config: dict):\n        self.config = config\n        self.img_size = config[\"img_size\"]\n        self.batch_size = config[\"batch_size\"]\n        self.model = None\n        self.class_names = None\n\n    def _build_augmenter(self):\n        return keras.Sequential([\n            layers.RandomFlip(mode=\"horizontal_and_vertical\"),\n            layers.RandomRotation(factor=30/360),\n            layers.RandomZoom(height_factor=(-0.2, 0.2), width_factor=(-0.2, 0.2)),\n            layers.RandomContrast(factor=0.2),\n            layers.RandomBrightness(factor=0.2),\n        ], name=\"augmenter\")\n\n    def _random_crop_resize(self, img, lbl):\n        size = self.img_size\n        bigger = int(size * 1.15)\n        img = tf.image.resize(img, (bigger, bigger))\n        img = tf.image.random_crop(img, size=[size, size, 3])\n        return img, lbl\n\n    def _preprocess_input(self, x):\n        return tf.keras.applications.mobilenet_v2.preprocess_input(x)\n\n    def _base_model(self):\n        input_shape = (self.img_size, self.img_size, 3)\n        base = tf.keras.applications.MobileNetV2(\n            include_top=False,\n            weights=\"imagenet\",\n            input_shape=input_shape,\n            alpha=self.config.get(\"mobilenet_alpha\", 1.0),\n        )\n        return base\n\n    def build_model(self, class_names: List[str]):\n        status(\"Building SMALL deployable model (MobileNetV2)...\")\n        self.class_names = class_names\n        num_classes = len(class_names)\n\n        inputs = keras.Input(shape=(self.img_size, self.img_size, 3), name=\"image\")\n        base = self._base_model()\n        base.trainable = False\n\n        x = self._preprocess_input(inputs)\n        x = base(x, training=False)\n\n        x = layers.GlobalAveragePooling2D()(x)\n        x = layers.Dense(\n            self.config[\"head_units\"],\n            kernel_regularizer=regularizers.l2(1e-4)\n        )(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Activation(\"relu\")(x)\n        x = layers.Dropout(self.config[\"dropout_rate\"])(x)\n\n        outputs = layers.Dense(\n            num_classes,\n            activation=\"softmax\",\n            dtype=\"float32\",\n            name=\"pred\"\n        )(x)\n\n        self.model = keras.Model(inputs=inputs, outputs=outputs, name=\"small_classifier\")\n        status(f\"Model built. Classes={num_classes}, alpha={self.config.get('mobilenet_alpha')}, head={self.config['head_units']}\")\n        return self.model\n\n    def compile_with_regularization(self, lr: float):\n        status(f\"Compiling model (lr={lr})...\")\n        try:\n            opt = tf.keras.optimizers.AdamW(\n                learning_rate=lr,\n                weight_decay=1e-4,\n                clipnorm=1.0\n            )\n        except Exception:\n            opt = tf.keras.optimizers.Adam(learning_rate=lr, clipnorm=1.0)\n\n        loss = tf.keras.losses.CategoricalCrossentropy(\n            label_smoothing=self.config[\"label_smoothing\"]\n        )\n\n        self.model.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n\n    def _mixup_batch(self, x, y):\n    # Fix mixed precision dtype mismatch:\n    # x can be float16 under mixed precision, y is float32 â†’ crash\n        x = tf.cast(x, tf.float32)\n        y = tf.cast(y, tf.float32)\n    \n        alpha = self.config[\"mixup_alpha\"]\n        if alpha <= 0:\n            return x, y\n    \n        lam = tf.cast(tf.random.gamma([], alpha, beta=1.0) /\n                      (tf.random.gamma([], alpha, beta=1.0) + tf.random.gamma([], alpha, beta=1.0)),\n                      tf.float32)\n    \n        bs = tf.shape(x)[0]\n        idx = tf.random.shuffle(tf.range(bs))\n        x2 = tf.gather(x, idx)\n        y2 = tf.gather(y, idx)\n    \n        x_mix = lam * x + (1.0 - lam) * x2\n        y_mix = lam * y + (1.0 - lam) * y2\n        return x_mix, y_mix\n\n\n    def _make_dataset(self, files: List[str], labels_oh: np.ndarray, training: bool):\n        ds = tf.data.Dataset.from_tensor_slices((files, labels_oh))\n\n        def _load(path, y):\n            img = tf.io.read_file(path)\n            img = tf.image.decode_image(img, channels=3, expand_animations=False)\n            img = tf.image.resize(img, (self.img_size, self.img_size))\n            img = tf.cast(img, tf.float32)\n            return img, y\n\n        ds = ds.map(_load, num_parallel_calls=self.config[\"num_workers\"])\n\n        if training:\n            aug = self._build_augmenter()\n            ds = ds.map(self._random_crop_resize, num_parallel_calls=self.config[\"num_workers\"])\n            ds = ds.map(lambda x, y: (aug(x, training=True), y),\n                        num_parallel_calls=self.config[\"num_workers\"])\n\n            ds = ds.shuffle(2048, seed=self.config[\"seed\"], reshuffle_each_iteration=True)\n\n            if self.config[\"mixup\"]:\n                ds = ds.batch(self.batch_size, drop_remainder=True)\n                ds = ds.map(self._mixup_batch, num_parallel_calls=self.config[\"num_workers\"])\n            else:\n                ds = ds.batch(self.batch_size)\n        else:\n            ds = ds.batch(self.batch_size)\n\n        return ds.prefetch(self.config[\"num_workers\"])\n\n    def train_with_callbacks(self, train_ds, val_ds, phase: str):\n        status(f\"Training {phase}...\")\n\n        callbacks = [\n            keras.callbacks.EarlyStopping(\n                monitor=\"val_loss\",\n                patience=self.config[\"early_stop_patience\"],\n                restore_best_weights=True\n            ),\n            keras.callbacks.ReduceLROnPlateau(\n                monitor=\"val_loss\",\n                factor=0.5,\n                patience=5,\n                min_lr=1e-7,\n                verbose=1\n            ),\n            keras.callbacks.ModelCheckpoint(\n                filepath=str(BEST_CKPT_PATH),\n                monitor=\"val_loss\",\n                save_best_only=True,\n                save_weights_only=True,\n                verbose=1\n            ),\n        ]\n        if phase == \"phase1\" and self.config[\"warmup_epochs\"] > 0:\n            callbacks.append(WarmupThenConstant(self.config[\"learning_rate\"], self.config[\"warmup_epochs\"]))\n\n        epochs = self.config[\"epochs_phase1\"] if phase == \"phase1\" else self.config[\"epochs_phase2\"]\n\n        hist = self.model.fit(\n            train_ds,\n            validation_data=val_ds,\n            epochs=epochs,\n            callbacks=callbacks\n        )\n        return hist\n\n    def evaluate(self, test_ds, y_true_idx: np.ndarray, class_names: List[str]):\n        status(\"Evaluating on test set...\")\n        if BEST_CKPT_PATH.exists():\n            self.model.load_weights(str(BEST_CKPT_PATH))\n            status(\"Loaded best checkpoint for evaluation.\")\n\n        y_prob = self.model.predict(test_ds, verbose=1)\n        y_pred = np.argmax(y_prob, axis=1)\n\n        cm = confusion_matrix(y_true_idx, y_pred)\n        report = classification_report(\n            y_true_idx, y_pred,\n            target_names=class_names,\n            output_dict=True\n        )\n\n        per_class_acc = {}\n        for i, name in enumerate(class_names):\n            denom = cm[i].sum()\n            per_class_acc[name] = float(cm[i, i] / denom) if denom > 0 else 0.0\n\n        thr = self.config[\"unknown_threshold\"]\n        maxp = np.max(y_prob, axis=1)\n        unknown_rate = float(np.mean(maxp < thr))\n\n        return {\n            \"confusion_matrix\": cm.tolist(),\n            \"classification_report\": report,\n            \"per_class_accuracy\": per_class_acc,\n            \"unknown_threshold\": thr,\n            \"unknown_rate_on_test\": unknown_rate\n        }\n\n# =========================================================\n# Split helpers\n# =========================================================\ndef build_file_label_lists(root: Path) -> Tuple[List[str], List[str]]:\n    classes = sorted([d.name for d in root.iterdir() if d.is_dir()])\n    files, labels = [], []\n    for c in classes:\n        imgs = list_images(root / c)\n        for fp in imgs:\n            files.append(str(fp))\n            labels.append(c)\n    return files, labels\n\ndef export_split_to_folders(split_name: str, files: List[str], labels: List[str], out_root: Path):\n    status(f\"Exporting split '{split_name}'...\")\n    if out_root.exists():\n        shutil.rmtree(out_root, ignore_errors=True)\n    safe_mkdir(out_root)\n\n    for fp, cls in tqdm(list(zip(files, labels)), desc=f\"Copying {split_name}\"):\n        try:\n            src = Path(fp)\n            dst_dir = out_root / cls\n            safe_mkdir(dst_dir)\n            shutil.copy2(src, dst_dir / src.name)\n        except Exception:\n            continue\n\ndef make_test_zip(test_dir: Path, zip_path: Path):\n    status(f\"Creating TEST.zip from: {test_dir}\")\n    zip_folder(test_dir, zip_path)\n    status(f\"TEST.zip created: {zip_path} ({zip_path.stat().st_size / (1024*1024):.2f} MB)\")\n\ndef one_hot(labels: List[str], class_to_idx: Dict[str, int]) -> np.ndarray:\n    y = np.zeros((len(labels), len(class_to_idx)), dtype=np.float32)\n    for i, c in enumerate(labels):\n        y[i, class_to_idx[c]] = 1.0\n    return y\n\n# =========================================================\n# TFLite export\n# =========================================================\ndef export_tflite_fp16(model: tf.keras.Model):\n    status(\"Exporting TFLite FP16...\")\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    converter.target_spec.supported_types = [tf.float16]\n    tflite_model = converter.convert()\n    TFLITE_FP16_PATH.write_bytes(tflite_model)\n    status(f\"Saved: {TFLITE_FP16_PATH} ({TFLITE_FP16_PATH.stat().st_size/(1024*1024):.2f} MB)\")\n\ndef export_tflite_int8(model: tf.keras.Model, rep_paths: List[str], img_size: int):\n    status(\"Exporting TFLite INT8 (smallest)...\")\n\n    def representative_dataset():\n        for p in rep_paths:\n            try:\n                img = tf.io.read_file(p)\n                img = tf.image.decode_image(img, channels=3, expand_animations=False)\n                img = tf.image.resize(img, (img_size, img_size))\n                img = tf.cast(img, tf.float32)\n                img = tf.expand_dims(img, axis=0)\n                yield [img]\n            except Exception:\n                continue\n\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    converter.representative_dataset = representative_dataset\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n    converter.inference_input_type = tf.uint8\n    converter.inference_output_type = tf.uint8\n\n    tflite_model = converter.convert()\n    TFLITE_INT8_PATH.write_bytes(tflite_model)\n    status(f\"Saved: {TFLITE_INT8_PATH} ({TFLITE_INT8_PATH.stat().st_size/(1024*1024):.2f} MB)\")\n\n# =========================================================\n# main\n# =========================================================\ndef main():\n    try:\n        set_seeds(CONFIG[\"seed\"])\n        enable_gpu_optimizations()\n        maybe_enable_mixed_precision()\n\n        status(\"Starting training pipeline on your Kaggle dataset structure...\")\n        status(f\"DATA_DIR: {DATA_DIR}\")\n        if not DATA_DIR.exists():\n            raise FileNotFoundError(f\"DATA_DIR not found: {DATA_DIR}\")\n\n        # Quick folder existence check\n        status(\"Checking class source folders...\")\n        for k, v in CLASS_SOURCES.items():\n            print(f\"  {k:12s} -> exists={v.exists()} path={v}\")\n\n        # 1) Balance (creates BALANCED_DIR/class_name/*.jpg)\n        balancer = DataBalancer(BALANCED_DIR, CONFIG)\n        balance_report = balancer.balance_classes()\n\n        # 2) Stratified splits\n        status(\"Creating stratified splits from balanced dataset...\")\n        files, labels = build_file_label_lists(BALANCED_DIR)\n        classes = sorted(list(set(labels)))\n        class_to_idx = {c: i for i, c in enumerate(classes)}\n        save_json(CLASS_MAP_PATH, {\"classes\": classes, \"class_to_idx\": class_to_idx})\n        status(f\"Saved class mapping: {CLASS_MAP_PATH}\")\n\n        X_trainval, X_test, y_trainval, y_test = train_test_split(\n            files, labels,\n            test_size=CONFIG[\"test_split\"],\n            random_state=CONFIG[\"seed\"],\n            stratify=labels\n        )\n\n        val_relative = CONFIG[\"val_split\"] / (1.0 - CONFIG[\"test_split\"])\n        X_train, X_val, y_train, y_val = train_test_split(\n            X_trainval, y_trainval,\n            test_size=val_relative,\n            random_state=CONFIG[\"seed\"],\n            stratify=y_trainval\n        )\n\n        status(f\"Split sizes: train={len(X_train)}, val={len(X_val)}, test={len(X_test)}\")\n\n        export_split_to_folders(\"train\", X_train, y_train, TRAIN_DIR)\n        export_split_to_folders(\"val\", X_val, y_val, VAL_DIR)\n        export_split_to_folders(\"test\", X_test, y_test, TEST_DIR)\n\n        make_test_zip(TEST_DIR, TEST_ZIP_PATH)\n\n        # rebuild split lists (ensures no leakage)\n        train_files, train_labels = build_file_label_lists(TRAIN_DIR)\n        val_files, val_labels = build_file_label_lists(VAL_DIR)\n        test_files, test_labels = build_file_label_lists(TEST_DIR)\n\n        y_train_oh = one_hot(train_labels, class_to_idx)\n        y_val_oh = one_hot(val_labels, class_to_idx)\n        y_test_oh = one_hot(test_labels, class_to_idx)\n\n        # 3) Build + Train\n        trainer = AntiOverfitTrainer(CONFIG)\n        model = trainer.build_model(classes)\n\n        train_ds = trainer._make_dataset(train_files, y_train_oh, training=True)\n        val_ds = trainer._make_dataset(val_files, y_val_oh, training=False)\n        test_ds = trainer._make_dataset(test_files, y_test_oh, training=False)\n\n        # Resume support\n        trainer.compile_with_regularization(CONFIG[\"learning_rate\"])\n        if BEST_CKPT_PATH.exists():\n            warn(\"Checkpoint found. Loading weights to resume.\")\n            model.load_weights(str(BEST_CKPT_PATH))\n\n        hist_all = {\"loss\": [], \"accuracy\": [], \"val_loss\": [], \"val_accuracy\": []}\n\n        # Phase 1\n        h1 = trainer.train_with_callbacks(train_ds, val_ds, \"phase1\")\n        for k in hist_all:\n            if k in h1.history:\n                hist_all[k].extend(h1.history[k])\n\n        # Phase 2: unfreeze top 30%\n        status(\"Fine-tuning: unfreezing top 30% layers...\")\n        base_model = None\n        for layer in model.layers:\n            if isinstance(layer, tf.keras.Model) and \"mobilenet\" in layer.name.lower():\n                base_model = layer\n                break\n\n        if base_model is not None:\n            base_model.trainable = True\n            n_layers = len(base_model.layers)\n            cut = int(n_layers * 0.7)\n            for i, layer in enumerate(base_model.layers):\n                layer.trainable = (i >= cut)\n            status(f\"Base layers: {n_layers}, trainable: {n_layers - cut}, frozen: {cut}\")\n        else:\n            warn(\"Could not find MobileNet base model block; fine-tune may be limited.\")\n\n        trainer.compile_with_regularization(CONFIG[\"fine_tune_lr\"])\n        if BEST_CKPT_PATH.exists():\n            model.load_weights(str(BEST_CKPT_PATH))\n            status(\"Loaded best checkpoint before fine-tune.\")\n\n        h2 = trainer.train_with_callbacks(train_ds, val_ds, \"phase2\")\n        for k in hist_all:\n            if k in h2.history:\n                hist_all[k].extend(h2.history[k])\n\n        # Save history CSV\n        status(\"Saving history CSV...\")\n        with open(HISTORY_CSV_PATH, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n            writer = csv.writer(f)\n            writer.writerow(list(hist_all.keys()))\n            for i in range(len(hist_all[\"loss\"])):\n                writer.writerow([hist_all[k][i] for k in hist_all])\n\n        # Overfit indicator\n        gaps = [float(a - b) for a, b in zip(hist_all[\"accuracy\"], hist_all[\"val_accuracy\"])]\n        overfit_summary = {\n            \"max_train_val_acc_gap\": float(np.max(gaps)) if gaps else None,\n            \"mean_train_val_acc_gap\": float(np.mean(gaps)) if gaps else None\n        }\n        status(f\"Overfit gap summary: {overfit_summary}\")\n\n        # Evaluate\n        y_test_idx = np.array([class_to_idx[c] for c in test_labels], dtype=np.int32)\n        eval_out = trainer.evaluate(test_ds, y_test_idx, classes)\n\n        # Save model\n        status(\"Saving best model (.h5 + SavedModel)...\")\n        if BEST_CKPT_PATH.exists():\n            model.load_weights(str(BEST_CKPT_PATH))\n\n        model.save(str(BEST_H5_PATH), include_optimizer=False)\n        if SAVEDMODEL_PATH.exists():\n            shutil.rmtree(SAVEDMODEL_PATH, ignore_errors=True)\n        model.save(str(SAVEDMODEL_PATH))\n\n        status(f\"Saved .h5: {BEST_H5_PATH} ({BEST_H5_PATH.stat().st_size/(1024*1024):.2f} MB)\")\n\n        # TFLite exports\n        if CONFIG[\"export_tflite\"]:\n            export_tflite_fp16(model)\n            if CONFIG[\"export_int8\"]:\n                rep_n = min(CONFIG[\"rep_data_samples\"], len(train_files))\n                rep_paths = random.sample(train_files, rep_n) if rep_n > 0 else train_files[:rep_n]\n                export_tflite_int8(model, rep_paths, CONFIG[\"img_size\"])\n\n        # Summary report\n        summary = {\n            \"config\": CONFIG,\n            \"classes\": classes,\n            \"class_mapping\": str(CLASS_MAP_PATH),\n            \"balance_report\": str(BALANCE_REPORT_PATH),\n            \"history_csv\": str(HISTORY_CSV_PATH),\n            \"test_zip\": str(TEST_ZIP_PATH),\n            \"models\": {\n                \"best_h5\": str(BEST_H5_PATH),\n                \"saved_model\": str(SAVEDMODEL_PATH),\n                \"best_weights\": str(BEST_CKPT_PATH),\n                \"tflite_fp16\": str(TFLITE_FP16_PATH) if TFLITE_FP16_PATH.exists() else None,\n                \"tflite_int8\": str(TFLITE_INT8_PATH) if TFLITE_INT8_PATH.exists() else None,\n            },\n            \"overfit_indicators\": overfit_summary,\n            \"evaluation\": eval_out\n        }\n        save_json(SUMMARY_REPORT_PATH, summary)\n        status(f\"Saved summary: {SUMMARY_REPORT_PATH}\")\n\n        # Zip models for download\n        models_zip = ZIP_DIR / \"models_bundle.zip\"\n        status(\"Zipping MODELS_DIR for easy download...\")\n        zip_folder(MODELS_DIR, models_zip)\n        status(f\"Download: {models_zip} ({models_zip.stat().st_size/(1024*1024):.2f} MB)\")\n\n        status(\"âœ… DONE. Go to Kaggle Output panel and download run_output/zips/models_bundle.zip + TEST.zip\")\n\n    except Exception as e:\n        err(\"Pipeline crashed ðŸ˜­\")\n        print(\"Error:\", e)\n        print(traceback.format_exc())\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T17:21:45.368872Z","iopub.execute_input":"2026-02-03T17:21:45.369421Z","iopub.status.idle":"2026-02-03T17:35:16.018872Z","shell.execute_reply.started":"2026-02-03T17:21:45.369387Z","shell.execute_reply":"2026-02-03T17:35:16.018234Z"}},"outputs":[{"name":"stdout","text":"\nðŸŸ¦ GPU detected: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n\nðŸŸ¦ Mixed precision enabled (fp16).\n\nðŸŸ¦ Starting training pipeline on your Kaggle dataset structure...\n\nðŸŸ¦ DATA_DIR: /kaggle/input/procesed-again-costal-polutant\n\nðŸŸ¦ Checking class source folders...\n  cardboard    -> exists=True path=/kaggle/input/procesed-again-costal-polutant/cardboard\n  glass        -> exists=True path=/kaggle/input/procesed-again-costal-polutant/glass\n  metal        -> exists=True path=/kaggle/input/procesed-again-costal-polutant/metal\n  paper        -> exists=True path=/kaggle/input/procesed-again-costal-polutant/paper\n  plastic      -> exists=True path=/kaggle/input/procesed-again-costal-polutant/plastic\n  marine_trash -> exists=True path=/kaggle/input/procesed-again-costal-polutant/processed_marine_waste/marine_trash\n  oil_spill    -> exists=True path=/kaggle/input/procesed-again-costal-polutant/processed_oilspill/oil_spill\n\nðŸŸ¦ Analyzing class distribution from CLASS_SOURCES...\nClass distribution (BEFORE):\n  - cardboard: 1533\n  - glass: 1998\n  - metal: 993\n  - paper: 1380\n  - plastic: 1709\n  - marine_trash: 7683\n  - oil_spill: 768\nTotal images: 16064\n\nðŸŸ¦ Balancing target per class: 768 images\n\nðŸŸ¦ 'cardboard': 1533 -> keeping 768 (random deletion)\n","output_type":"stream"},{"name":"stderr","text":"Copying cardboard: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 768/768 [00:01<00:00, 618.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸŸ¦ 'glass': 1998 -> keeping 768 (random deletion)\n","output_type":"stream"},{"name":"stderr","text":"Copying glass: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 768/768 [00:01<00:00, 630.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸŸ¦ 'metal': 993 -> keeping 768 (random deletion)\n","output_type":"stream"},{"name":"stderr","text":"Copying metal: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 768/768 [00:01<00:00, 613.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸŸ¦ 'paper': 1380 -> keeping 768 (random deletion)\n","output_type":"stream"},{"name":"stderr","text":"Copying paper: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 768/768 [00:01<00:00, 595.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸŸ¦ 'plastic': 1709 -> keeping 768 (random deletion)\n","output_type":"stream"},{"name":"stderr","text":"Copying plastic: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 768/768 [00:01<00:00, 584.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸŸ¦ 'marine_trash': 7683 -> keeping 768 (random deletion)\n","output_type":"stream"},{"name":"stderr","text":"Copying marine_trash: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 768/768 [00:01<00:00, 597.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸŸ¦ 'oil_spill': 768 -> boosting to 768 (offline augmentation)\n","output_type":"stream"},{"name":"stderr","text":"Copying oil_spill: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 768/768 [00:01<00:00, 513.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸŸ¦ Balancing report saved to: /kaggle/working/run_output/reports/balancing_report.json\nClass distribution (AFTER):\n  - cardboard: 768\n  - glass: 768\n  - metal: 768\n  - paper: 768\n  - plastic: 768\n  - marine_trash: 768\n  - oil_spill: 768\n\nðŸŸ¦ Creating stratified splits from balanced dataset...\n\nðŸŸ¦ Saved class mapping: /kaggle/working/run_output/models/class_mapping.json\n\nðŸŸ¦ Split sizes: train=3762, val=807, test=807\n\nðŸŸ¦ Exporting split 'train'...\n","output_type":"stream"},{"name":"stderr","text":"Copying train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3762/3762 [00:00<00:00, 6354.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸŸ¦ Exporting split 'val'...\n","output_type":"stream"},{"name":"stderr","text":"Copying val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 807/807 [00:00<00:00, 6598.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸŸ¦ Exporting split 'test'...\n","output_type":"stream"},{"name":"stderr","text":"Copying test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 807/807 [00:00<00:00, 6383.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸŸ¦ Creating TEST.zip from: /kaggle/working/run_output/splits/test\n\nðŸŸ¦ TEST.zip created: /kaggle/working/run_output/TEST.zip (8.72 MB)\n\nðŸŸ¦ Building SMALL deployable model (MobileNetV2)...\n\nðŸŸ¦ Model built. Classes=7, alpha=0.75, head=128\n\nðŸŸ¦ Compiling model (lr=0.001)...\n\nðŸŸ¦ Training phase1...\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1770139353.942102     128 service.cc:152] XLA service 0x786edc216fc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1770139353.942146     128 service.cc:160]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1770139353.942151     128 service.cc:160]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1770139355.818711     128 cuda_dnn.cc:529] Loaded cuDNN version 91002\nI0000 00:00:1770139372.305169     128 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.3571 - loss: nan\nEpoch 1: val_loss did not improve from inf\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 330ms/step - accuracy: 0.3580 - loss: nan - val_accuracy: 0.1425 - val_loss: nan - learning_rate: 5.0000e-04\nEpoch 2/10\n\u001b[1m115/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.5606 - loss: nan\nEpoch 2: val_loss did not improve from inf\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 127ms/step - accuracy: 0.5616 - loss: nan - val_accuracy: 0.1425 - val_loss: nan - learning_rate: 0.0010\nEpoch 3/10\n\u001b[1m116/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.6077 - loss: nan\nEpoch 3: val_loss did not improve from inf\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 128ms/step - accuracy: 0.6082 - loss: nan - val_accuracy: 0.1425 - val_loss: nan - learning_rate: 0.0010\nEpoch 4/10\n\u001b[1m115/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.6206 - loss: nan\nEpoch 4: val_loss did not improve from inf\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 127ms/step - accuracy: 0.6215 - loss: nan - val_accuracy: 0.1425 - val_loss: nan - learning_rate: 0.0010\nEpoch 5/10\n\u001b[1m116/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.6519 - loss: nan\nEpoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\nEpoch 5: val_loss did not improve from inf\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 126ms/step - accuracy: 0.6522 - loss: nan - val_accuracy: 0.1425 - val_loss: nan - learning_rate: 0.0010\nEpoch 6/10\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.6453 - loss: nan\nEpoch 6: val_loss did not improve from inf\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 130ms/step - accuracy: 0.6455 - loss: nan - val_accuracy: 0.1425 - val_loss: nan - learning_rate: 0.0010\nEpoch 7/10\n\u001b[1m116/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.6344 - loss: nan\nEpoch 7: val_loss did not improve from inf\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 130ms/step - accuracy: 0.6350 - loss: nan - val_accuracy: 0.1425 - val_loss: nan - learning_rate: 0.0010\nEpoch 8/10\n\u001b[1m116/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.6509 - loss: nan\nEpoch 8: val_loss did not improve from inf\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 128ms/step - accuracy: 0.6513 - loss: nan - val_accuracy: 0.1425 - val_loss: nan - learning_rate: 0.0010\nEpoch 9/10\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.6477 - loss: nan\nEpoch 9: val_loss did not improve from inf\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 127ms/step - accuracy: 0.6480 - loss: nan - val_accuracy: 0.1425 - val_loss: nan - learning_rate: 0.0010\nEpoch 10/10\n\u001b[1m116/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.6324 - loss: nan\nEpoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n\nEpoch 10: val_loss did not improve from inf\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 132ms/step - accuracy: 0.6330 - loss: nan - val_accuracy: 0.1425 - val_loss: nan - learning_rate: 0.0010\n\nðŸŸ¦ Fine-tuning: unfreezing top 30% layers...\n\nðŸŸ¦ Base layers: 154, trainable: 47, frozen: 107\n\nðŸŸ¦ Compiling model (lr=1e-05)...\n\nðŸŸ¦ Training phase2...\nEpoch 1/30\n\u001b[1m116/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - accuracy: 0.4644 - loss: nan\nEpoch 1: val_loss did not improve from inf\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 285ms/step - accuracy: 0.4652 - loss: nan - val_accuracy: 0.1425 - val_loss: nan - learning_rate: 1.0000e-05\nEpoch 2/30\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.5121 - loss: nan\nEpoch 2: val_loss did not improve from inf\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 131ms/step - accuracy: 0.5125 - loss: nan - val_accuracy: 0.1425 - val_loss: nan - learning_rate: 1.0000e-05\nEpoch 3/30\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - accuracy: 0.5270 - loss: nan\nEpoch 3: val_loss did not improve from inf\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 133ms/step - accuracy: 0.5274 - loss: nan - val_accuracy: 0.1425 - val_loss: nan - learning_rate: 1.0000e-05\nEpoch 4/30\n\u001b[1m116/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.5204 - loss: nan\nEpoch 4: val_loss did not improve from inf\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 131ms/step - accuracy: 0.5214 - loss: nan - val_accuracy: 0.1425 - val_loss: nan - learning_rate: 1.0000e-05\nEpoch 5/30\n\u001b[1m116/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.5502 - loss: nan\nEpoch 5: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n\nEpoch 5: val_loss did not improve from inf\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 131ms/step - accuracy: 0.5510 - loss: nan - val_accuracy: 0.1425 - val_loss: nan - learning_rate: 1.0000e-05\nEpoch 6/30\n\u001b[1m116/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.5663 - loss: nan\nEpoch 6: val_loss did not improve from inf\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 131ms/step - accuracy: 0.5669 - loss: nan - val_accuracy: 0.1425 - val_loss: nan - learning_rate: 5.0000e-06\nEpoch 7/30\n\u001b[1m115/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.5490 - loss: nan\nEpoch 7: val_loss did not improve from inf\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 133ms/step - accuracy: 0.5502 - loss: nan - val_accuracy: 0.1425 - val_loss: nan - learning_rate: 5.0000e-06\nEpoch 8/30\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.5630 - loss: nan\nEpoch 8: val_loss did not improve from inf\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 131ms/step - accuracy: 0.5633 - loss: nan - val_accuracy: 0.1425 - val_loss: nan - learning_rate: 5.0000e-06\nEpoch 9/30\n\u001b[1m116/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.5806 - loss: nan\nEpoch 9: val_loss did not improve from inf\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 131ms/step - accuracy: 0.5811 - loss: nan - val_accuracy: 0.1425 - val_loss: nan - learning_rate: 5.0000e-06\nEpoch 10/30\n\u001b[1m115/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.5544 - loss: nan\nEpoch 10: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-06.\n\nEpoch 10: val_loss did not improve from inf\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 130ms/step - accuracy: 0.5558 - loss: nan - val_accuracy: 0.1425 - val_loss: nan - learning_rate: 5.0000e-06\nEpoch 11/30\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - accuracy: 0.5860 - loss: nan\nEpoch 11: val_loss did not improve from inf\n\u001b[1m117/117\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 131ms/step - accuracy: 0.5863 - loss: nan - val_accuracy: 0.1425 - val_loss: nan - learning_rate: 2.5000e-06\n\nðŸŸ¦ Saving history CSV...\n\nðŸŸ¦ Overfit gap summary: {'max_train_val_acc_gap': 0.5353815406560898, 'mean_train_val_acc_gap': 0.4689387033383052}\n\nðŸŸ¦ Evaluating on test set...\n\u001b[1m26/26\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 313ms/step\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\nWARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\nðŸŸ¦ Saving best model (.h5 + SavedModel)...\nðŸŸ¥ Pipeline crashed ðŸ˜­\nError: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=/kaggle/working/run_output/models/saved_model.\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_55/1363905620.py\", line 827, in main\n    model.save(str(SAVEDMODEL_PATH))\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_api.py\", line 114, in save_model\n    raise ValueError(\nValueError: Invalid filepath extension for saving. Please add either a `.keras` extension for the native Keras format (recommended) or a `.h5` extension. Use `model.export(filepath)` if you want to export a SavedModel for use with TFLite/TFServing/etc. Received: filepath=/kaggle/working/run_output/models/saved_model.\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os, json, csv, shutil, zipfile, random, math\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nfrom tqdm import tqdm\nfrom PIL import Image, ImageEnhance, ImageOps, ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers, regularizers\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nimport matplotlib.pyplot as plt\n\n# =========================\n# CONFIG (minimal + stable)\n# =========================\nCONFIG = {\n    \"seed\": 42,\n    \"img_size\": 224,\n    \"batch_size\": 32,\n\n    \"epochs\": 20,\n    \"fine_tune_epochs\": 8,\n\n    \"learning_rate\": 3e-4,\n    \"fine_tune_lr\": 1e-5,\n\n    \"dropout\": 0.35,\n    \"l2\": 1e-4,\n    \"label_smoothing\": 0.05,\n\n    \"val_split\": 0.15,\n    \"test_split\": 0.15,\n\n    \"early_stop_patience\": 6,\n    \"reduce_lr_patience\": 3,\n\n    # ===== Balancing rules =====\n    \"balance_target\": \"median\",       # \"median\" or \"min\" or \"fixed\"\n    \"fixed_target\": 800,              # used only if balance_target=\"fixed\"\n    \"max_target_cap\": 1200,           # donâ€™t inflate dataset too much\n    \"allow_deletion\": True,           # delete ONLY if class >> target * delete_factor\n    \"delete_factor\": 1.8,             # if class_count > target*1.8 -> downsample to target\n    \"max_delete_fraction\": 0.6,       # never delete more than 60% of a class\n\n    # model size\n    \"mobilenet_alpha\": 0.75,          # small model\n}\n\n# =========================\n# DATASET PATH (your Kaggle structure)\n# =========================\nDATA_DIR = Path(\"/kaggle/input/procesed-again-costal-polutant\")\n\nCLASS_SOURCES = {\n    \"cardboard\": DATA_DIR / \"cardboard\",\n    \"glass\": DATA_DIR / \"glass\",\n    \"metal\": DATA_DIR / \"metal\",\n    \"paper\": DATA_DIR / \"paper\",\n    \"plastic\": DATA_DIR / \"plastic\",\n    \"marine_trash\": DATA_DIR / \"processed_marine_waste\" / \"marine_trash\",\n    \"oil_spill\": DATA_DIR / \"processed_oilspill\" / \"oil_spill\",\n}\n\n# =========================\n# OUTPUTS\n# =========================\nRUN_DIR = Path(\"/kaggle/working/run_min_balance\")\nBALANCED_DIR = RUN_DIR / \"balanced_dataset\"\nSPLITS_DIR = RUN_DIR / \"splits\"\nTRAIN_DIR = SPLITS_DIR / \"train\"\nVAL_DIR = SPLITS_DIR / \"val\"\nTEST_DIR = SPLITS_DIR / \"test\"\n\nMODELS_DIR = RUN_DIR / \"models\"\nPLOTS_DIR = RUN_DIR / \"plots\"\nREPORTS_DIR = RUN_DIR / \"reports\"\n\nfor d in [RUN_DIR, BALANCED_DIR, SPLITS_DIR, MODELS_DIR, PLOTS_DIR, REPORTS_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n\nBEST_WEIGHTS = MODELS_DIR / \"best.weights.h5\"\nMODEL_H5 = MODELS_DIR / \"model_best.h5\"\nMODEL_KERAS = MODELS_DIR / \"model_best.keras\"\nSAVEDMODEL_DIR = MODELS_DIR / \"saved_model\"\nCLASS_MAP = MODELS_DIR / \"class_mapping.json\"\nTEST_ZIP = RUN_DIR / \"TEST.zip\"\n\nCOUNTS_JSON = REPORTS_DIR / \"class_counts.json\"\nBALANCE_REPORT_JSON = REPORTS_DIR / \"balance_report.json\"\nHISTORY_CSV = REPORTS_DIR / \"history.csv\"\nCLASS_REPORT_JSON = REPORTS_DIR / \"classification_report.json\"\nCONFUSION_JSON = REPORTS_DIR / \"confusion_matrix.json\"\n\n\n# =========================\n# Utils\n# =========================\ndef set_seeds(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\ndef status(msg): print(f\"\\nðŸŸ¦ {msg}\")\ndef warn(msg): print(f\"ðŸŸ¨ {msg}\")\n\ndef list_images(folder: Path):\n    exts = {\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\", \".tif\", \".tiff\"}\n    out = []\n    for fp in folder.rglob(\"*\"):\n        if fp.is_file() and fp.suffix.lower() in exts:\n            out.append(fp)\n    return out\n\ndef save_json(path: Path, obj: dict):\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(obj, f, indent=2)\n\ndef zip_folder(folder: Path, zip_path: Path):\n    if zip_path.exists():\n        zip_path.unlink()\n    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n        for fp in folder.rglob(\"*\"):\n            if fp.is_file():\n                zf.write(fp, arcname=str(fp.relative_to(folder)))\n\ndef enable_gpu_memory_growth():\n    gpus = tf.config.list_physical_devices(\"GPU\")\n    if gpus:\n        status(f\"GPU detected: {gpus}\")\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    else:\n        warn(\"No GPU detected.\")\n\ndef resize_to_square(im: Image.Image, size: int):\n    return im.resize((size, size), Image.BILINEAR)\n\ndef random_augment_pil(im: Image.Image):\n    # Minimal but effective augmentation\n    if random.random() < 0.5:\n        im = ImageOps.mirror(im)\n    if random.random() < 0.2:\n        im = ImageOps.flip(im)\n\n    angle = random.uniform(-20, 20)\n    im = im.rotate(angle, resample=Image.BILINEAR, expand=False)\n\n    if random.random() < 0.6:\n        im = ImageEnhance.Brightness(im).enhance(random.uniform(0.85, 1.15))\n    if random.random() < 0.6:\n        im = ImageEnhance.Contrast(im).enhance(random.uniform(0.85, 1.15))\n    return im\n\n\n# =========================\n# Step 1: Count images\n# =========================\ndef count_images_per_class():\n    status(\"Counting images per class...\")\n    counts = {}\n    for cls, folder in CLASS_SOURCES.items():\n        if not folder.exists():\n            warn(f\"Missing: {folder}\")\n            counts[cls] = 0\n            continue\n        counts[cls] = len(list_images(folder))\n\n    total = sum(counts.values())\n    for cls in sorted(counts.keys()):\n        print(f\"  - {cls:12s}: {counts[cls]}\")\n    print(f\"Total images: {total}\")\n\n    save_json(COUNTS_JSON, counts)\n    status(f\"Saved counts: {COUNTS_JSON}\")\n    return counts\n\n\n# =========================\n# Step 2: Minimal balance (augment only, delete only if extreme)\n# =========================\ndef choose_target(counts: Dict[str, int]) -> int:\n    valid = [v for v in counts.values() if v > 0]\n    if not valid:\n        raise ValueError(\"No images found in dataset.\")\n    if CONFIG[\"balance_target\"] == \"min\":\n        t = min(valid)\n    elif CONFIG[\"balance_target\"] == \"fixed\":\n        t = int(CONFIG[\"fixed_target\"])\n    else:\n        t = int(np.median(valid))\n    t = min(t, CONFIG[\"max_target_cap\"])\n    return max(t, 10)\n\ndef build_balanced_dataset(counts: Dict[str, int]):\n    status(\"Building balanced dataset (augment minority, delete only if necessary)...\")\n\n    # clean old\n    if BALANCED_DIR.exists():\n        shutil.rmtree(BALANCED_DIR, ignore_errors=True)\n    BALANCED_DIR.mkdir(parents=True, exist_ok=True)\n\n    target = choose_target(counts)\n    status(f\"Balance target per class: {target}\")\n\n    report = {\"target\": target, \"before\": counts, \"after\": {}, \"actions\": {}}\n\n    for cls, folder in CLASS_SOURCES.items():\n        out_cls = BALANCED_DIR / cls\n        out_cls.mkdir(parents=True, exist_ok=True)\n\n        imgs = list_images(folder) if folder.exists() else []\n        if len(imgs) == 0:\n            warn(f\"Class {cls} has 0 images, skipping.\")\n            report[\"after\"][cls] = 0\n            report[\"actions\"][cls] = {\"copied\": 0, \"augmented\": 0, \"deleted\": 0}\n            continue\n\n        random.shuffle(imgs)\n\n        # Decide if we need to delete\n        copied_list = imgs\n        deleted = 0\n        if CONFIG[\"allow_deletion\"] and len(imgs) > int(target * CONFIG[\"delete_factor\"]):\n            # don't delete more than max_delete_fraction\n            max_keep = int(len(imgs) * (1 - CONFIG[\"max_delete_fraction\"]))\n            keep = max(target, max_keep)\n            keep = min(keep, len(imgs))\n            copied_list = imgs[:keep]\n            deleted = len(imgs) - keep\n            status(f\"Downsampling '{cls}': {len(imgs)} -> {keep} (deleted={deleted})\")\n\n        # Copy originals (resized to jpg)\n        copied = 0\n        for i, fp in enumerate(tqdm(copied_list, desc=f\"Copy {cls}\")):\n            try:\n                im = Image.open(fp).convert(\"RGB\")\n                im = resize_to_square(im, CONFIG[\"img_size\"])\n                im.save(out_cls / f\"{cls}_{i:07d}.jpg\", quality=90, optimize=True)\n                copied += 1\n            except Exception:\n                continue\n\n        # Augment only if below target\n        augmented = 0\n        if copied < target:\n            need = target - copied\n            status(f\"Augmenting '{cls}' by {need} to reach {target}...\")\n            # Use the copied images as base for augmentation\n            base_paths = list(out_cls.glob(\"*.jpg\"))\n            if len(base_paths) == 0:\n                base_paths = [Path(p) for p in copied_list if p.exists()]\n\n            for j in tqdm(range(need), desc=f\"Aug {cls}\"):\n                try:\n                    base = random.choice(base_paths)\n                    im = Image.open(base).convert(\"RGB\")\n                    im = random_augment_pil(im)\n                    im = resize_to_square(im, CONFIG[\"img_size\"])\n                    im.save(out_cls / f\"aug_{cls}_{j:07d}.jpg\", quality=88, optimize=True)\n                    augmented += 1\n                except Exception:\n                    continue\n\n        after_count = len(list(out_cls.glob(\"*.jpg\")))\n        report[\"after\"][cls] = after_count\n        report[\"actions\"][cls] = {\"copied\": copied, \"augmented\": augmented, \"deleted\": deleted}\n\n    save_json(BALANCE_REPORT_JSON, report)\n    status(f\"Saved balance report: {BALANCE_REPORT_JSON}\")\n    return report\n\n\n# =========================\n# Split helpers\n# =========================\ndef build_file_label_lists_from_folder(root: Path, classes: List[str]):\n    files, labels = [], []\n    for cls in classes:\n        folder = root / cls\n        if not folder.exists():\n            continue\n        imgs = list_images(folder)\n        files.extend([str(p) for p in imgs])\n        labels.extend([cls] * len(imgs))\n    return files, labels\n\ndef export_split(split_name: str, files: list, labels: list, out_dir: Path):\n    status(f\"Exporting {split_name} split to: {out_dir}\")\n    if out_dir.exists():\n        shutil.rmtree(out_dir, ignore_errors=True)\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    for fp, cls in tqdm(list(zip(files, labels)), desc=f\"Copying {split_name}\"):\n        src = Path(fp)\n        dst = out_dir / cls\n        dst.mkdir(parents=True, exist_ok=True)\n        shutil.copy2(src, dst / src.name)\n\n\n# =========================\n# tf.data\n# =========================\ndef make_dataset(files, labels_idx, num_classes, training: bool):\n    img_size = CONFIG[\"img_size\"]\n    batch = CONFIG[\"batch_size\"]\n\n    y = tf.one_hot(labels_idx, depth=num_classes)\n    ds = tf.data.Dataset.from_tensor_slices((files, y))\n\n    def _load(path, y):\n        img = tf.io.read_file(path)\n        img = tf.image.decode_jpeg(img, channels=3)\n        img = tf.image.resize(img, (img_size, img_size))\n        img = tf.cast(img, tf.float32)\n        return img, y\n\n    ds = ds.map(_load, num_parallel_calls=tf.data.AUTOTUNE)\n\n    if training:\n        aug = keras.Sequential([\n            layers.RandomFlip(\"horizontal\"),\n            layers.RandomRotation(0.08),\n            layers.RandomZoom(0.15),\n            layers.RandomContrast(0.15),\n        ])\n        ds = ds.shuffle(2048, seed=CONFIG[\"seed\"], reshuffle_each_iteration=True)\n        ds = ds.map(lambda x, y: (aug(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n\n    ds = ds.batch(batch).prefetch(tf.data.AUTOTUNE)\n    return ds\n\n\n# =========================\n# Model\n# =========================\ndef build_model(num_classes: int):\n    status(\"Building MobileNetV2 (small + stable)...\")\n    inputs = keras.Input(shape=(CONFIG[\"img_size\"], CONFIG[\"img_size\"], 3))\n    x = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)\n\n    base = tf.keras.applications.MobileNetV2(\n        include_top=False,\n        weights=\"imagenet\",\n        input_shape=(CONFIG[\"img_size\"], CONFIG[\"img_size\"], 3),\n        alpha=CONFIG[\"mobilenet_alpha\"],\n    )\n    base.trainable = False\n\n    x = base(x, training=False)\n    x = layers.GlobalAveragePooling2D()(x)\n    x = layers.Dense(128, kernel_regularizer=regularizers.l2(CONFIG[\"l2\"]))(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n    x = layers.Dropout(CONFIG[\"dropout\"])(x)\n\n    outputs = layers.Dense(num_classes, activation=\"softmax\", dtype=\"float32\")(x)\n    model = keras.Model(inputs, outputs)\n    return model, base\n\ndef compile_model(model, lr):\n    opt = keras.optimizers.Adam(learning_rate=lr, clipnorm=1.0)\n    loss = keras.losses.CategoricalCrossentropy(label_smoothing=CONFIG[\"label_smoothing\"])\n    model.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n\n\n# =========================\n# Plots\n# =========================\ndef plot_history(history, out_dir: Path):\n    out_dir.mkdir(parents=True, exist_ok=True)\n\n    plt.figure()\n    plt.plot(history[\"accuracy\"], label=\"train_acc\")\n    plt.plot(history[\"val_accuracy\"], label=\"val_acc\")\n    plt.legend(); plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(\"Accuracy\")\n    plt.savefig(out_dir / \"accuracy.png\", dpi=150)\n    plt.close()\n\n    plt.figure()\n    plt.plot(history[\"loss\"], label=\"train_loss\")\n    plt.plot(history[\"val_loss\"], label=\"val_loss\")\n    plt.legend(); plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Loss\")\n    plt.savefig(out_dir / \"loss.png\", dpi=150)\n    plt.close()\n\ndef plot_confusion_matrix(cm, class_names, out_path: Path):\n    plt.figure(figsize=(9, 9))\n    plt.imshow(cm)\n    plt.title(\"Confusion Matrix\")\n    plt.xlabel(\"Pred\"); plt.ylabel(\"True\")\n    plt.colorbar()\n    plt.xticks(range(len(class_names)), class_names, rotation=45, ha=\"right\")\n    plt.yticks(range(len(class_names)), class_names)\n    plt.tight_layout()\n    plt.savefig(out_path, dpi=150)\n    plt.close()\n\ndef plot_per_class_acc(cm, class_names, out_path: Path):\n    per_acc = []\n    for i in range(len(class_names)):\n        denom = cm[i].sum()\n        per_acc.append(cm[i, i] / denom if denom > 0 else 0.0)\n\n    plt.figure(figsize=(10, 4))\n    plt.bar(class_names, per_acc)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"Per-class Accuracy\")\n    plt.tight_layout()\n    plt.savefig(out_path, dpi=150)\n    plt.close()\n\n\n# =========================\n# MAIN\n# =========================\ndef main():\n    set_seeds(CONFIG[\"seed\"])\n    enable_gpu_memory_growth()\n\n    counts = count_images_per_class()\n    build_balanced_dataset(counts)\n\n    classes = sorted(list(CLASS_SOURCES.keys()))\n    class_to_idx = {c: i for i, c in enumerate(classes)}\n    save_json(CLASS_MAP, {\"classes\": classes, \"class_to_idx\": class_to_idx})\n    status(f\"Saved class mapping: {CLASS_MAP}\")\n\n    # Build file list from balanced dataset\n    files, labels = build_file_label_lists_from_folder(BALANCED_DIR, classes)\n    labels_idx = np.array([class_to_idx[c] for c in labels], dtype=np.int32)\n\n    # Split stratified\n    status(\"Creating stratified splits...\")\n    X_trainval, X_test, y_trainval, y_test = train_test_split(\n        files, labels,\n        test_size=CONFIG[\"test_split\"],\n        random_state=CONFIG[\"seed\"],\n        stratify=labels\n    )\n    val_relative = CONFIG[\"val_split\"] / (1.0 - CONFIG[\"test_split\"])\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_trainval, y_trainval,\n        test_size=val_relative,\n        random_state=CONFIG[\"seed\"],\n        stratify=y_trainval\n    )\n\n    status(f\"Split sizes: train={len(X_train)} val={len(X_val)} test={len(X_test)}\")\n\n    export_split(\"train\", X_train, y_train, TRAIN_DIR)\n    export_split(\"val\", X_val, y_val, VAL_DIR)\n    export_split(\"test\", X_test, y_test, TEST_DIR)\n\n    status(\"Creating TEST.zip...\")\n    zip_folder(TEST_DIR, TEST_ZIP)\n    status(f\"TEST.zip created: {TEST_ZIP} ({TEST_ZIP.stat().st_size/(1024*1024):.2f} MB)\")\n\n    # Build datasets from exported splits\n    train_files, train_labels = build_file_label_lists_from_folder(TRAIN_DIR, classes)\n    val_files, val_labels = build_file_label_lists_from_folder(VAL_DIR, classes)\n    test_files, test_labels = build_file_label_lists_from_folder(TEST_DIR, classes)\n\n    y_train_idx = np.array([class_to_idx[c] for c in train_labels], dtype=np.int32)\n    y_val_idx = np.array([class_to_idx[c] for c in val_labels], dtype=np.int32)\n    y_test_idx = np.array([class_to_idx[c] for c in test_labels], dtype=np.int32)\n\n    train_ds = make_dataset(train_files, y_train_idx, len(classes), training=True)\n    val_ds = make_dataset(val_files, y_val_idx, len(classes), training=False)\n    test_ds = make_dataset(test_files, y_test_idx, len(classes), training=False)\n\n    model, base = build_model(len(classes))\n    compile_model(model, CONFIG[\"learning_rate\"])\n\n    callbacks = [\n        keras.callbacks.ModelCheckpoint(\n            filepath=str(BEST_WEIGHTS),\n            monitor=\"val_loss\",\n            save_best_only=True,\n            save_weights_only=True,\n            verbose=1\n        ),\n        keras.callbacks.EarlyStopping(\n            monitor=\"val_loss\",\n            patience=CONFIG[\"early_stop_patience\"],\n            restore_best_weights=True\n        ),\n        keras.callbacks.ReduceLROnPlateau(\n            monitor=\"val_loss\",\n            factor=0.5,\n            patience=CONFIG[\"reduce_lr_patience\"],\n            min_lr=1e-7,\n            verbose=1\n        )\n    ]\n\n    status(\"Training phase 1 (frozen backbone)...\")\n    h1 = model.fit(train_ds, validation_data=val_ds, epochs=CONFIG[\"epochs\"], callbacks=callbacks)\n\n    # Fine-tune top layers (simple)\n    status(\"Fine-tuning (unfreeze top 30%)...\")\n    base.trainable = True\n    n = len(base.layers)\n    cut = int(n * 0.7)\n    for i, layer in enumerate(base.layers):\n        layer.trainable = (i >= cut)\n\n    compile_model(model, CONFIG[\"fine_tune_lr\"])\n    h2 = model.fit(train_ds, validation_data=val_ds, epochs=CONFIG[\"fine_tune_epochs\"], callbacks=callbacks)\n\n    history = {\n        \"accuracy\": h1.history[\"accuracy\"] + h2.history[\"accuracy\"],\n        \"val_accuracy\": h1.history[\"val_accuracy\"] + h2.history[\"val_accuracy\"],\n        \"loss\": h1.history[\"loss\"] + h2.history[\"loss\"],\n        \"val_loss\": h1.history[\"val_loss\"] + h2.history[\"val_loss\"],\n    }\n\n    status(\"Saving history CSV...\")\n    with open(HISTORY_CSV, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n        w = csv.writer(f)\n        w.writerow(history.keys())\n        for i in range(len(history[\"loss\"])):\n            w.writerow([history[k][i] for k in history])\n\n    status(\"Saving graphs...\")\n    plot_history(history, PLOTS_DIR)\n\n    status(\"Evaluating on test set...\")\n    if BEST_WEIGHTS.exists():\n        model.load_weights(str(BEST_WEIGHTS))\n\n    y_prob = model.predict(test_ds, verbose=1)\n    y_pred = np.argmax(y_prob, axis=1)\n\n    cm = confusion_matrix(y_test_idx, y_pred)\n    save_json(CONFUSION_JSON, {\"confusion_matrix\": cm.tolist()})\n    plot_confusion_matrix(cm, classes, PLOTS_DIR / \"confusion_matrix.png\")\n    plot_per_class_acc(cm, classes, PLOTS_DIR / \"per_class_accuracy.png\")\n\n    rep = classification_report(y_test_idx, y_pred, target_names=classes, output_dict=True, zero_division=0)\n    save_json(CLASS_REPORT_JSON, rep)\n\n    status(\"Saving model...\")\n    model.save(str(MODEL_KERAS))\n    model.save(str(MODEL_H5))\n    if SAVEDMODEL_DIR.exists():\n        shutil.rmtree(SAVEDMODEL_DIR, ignore_errors=True)\n    model.export(str(SAVEDMODEL_DIR))  # Keras 3 safe\n\n    status(\"âœ… DONE.\")\n    print(\"Artifacts:\")\n    print(\" -\", RUN_DIR)\n    print(\" - Balanced dataset:\", BALANCED_DIR)\n    print(\" - TEST.zip:\", TEST_ZIP)\n    print(\" - Plots:\", PLOTS_DIR)\n    print(\" - Reports:\", REPORTS_DIR)\n    print(\" - Models:\", MODELS_DIR)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T17:44:20.262594Z","iopub.execute_input":"2026-02-03T17:44:20.263418Z","iopub.status.idle":"2026-02-03T18:14:57.337874Z","shell.execute_reply.started":"2026-02-03T17:44:20.263388Z","shell.execute_reply":"2026-02-03T18:14:57.337212Z"}},"outputs":[{"name":"stdout","text":"\nðŸŸ¦ GPU detected: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n\nðŸŸ¦ Counting images per class...\n  - cardboard   : 1533\n  - glass       : 1998\n  - marine_trash: 7683\n  - metal       : 993\n  - oil_spill   : 768\n  - paper       : 1380\n  - plastic     : 1709\nTotal images: 16064\n\nðŸŸ¦ Saved counts: /kaggle/working/run_min_balance/reports/class_counts.json\n\nðŸŸ¦ Building balanced dataset (augment minority, delete only if necessary)...\n\nðŸŸ¦ Balance target per class: 1200\n","output_type":"stream"},{"name":"stderr","text":"Copy cardboard: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1533/1533 [00:02<00:00, 587.26it/s]\nCopy glass: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1998/1998 [00:03<00:00, 601.90it/s]\nCopy metal: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 993/993 [00:01<00:00, 577.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸŸ¦ Augmenting 'metal' by 207 to reach 1200...\n","output_type":"stream"},{"name":"stderr","text":"Aug metal: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 207/207 [00:00<00:00, 310.07it/s]\nCopy paper: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1380/1380 [00:02<00:00, 546.55it/s]\nCopy plastic: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1709/1709 [00:02<00:00, 581.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸŸ¦ Downsampling 'marine_trash': 7683 -> 3073 (deleted=4610)\n","output_type":"stream"},{"name":"stderr","text":"Copy marine_trash: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3073/3073 [00:05<00:00, 559.23it/s]\nCopy oil_spill: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 768/768 [00:01<00:00, 507.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸŸ¦ Augmenting 'oil_spill' by 432 to reach 1200...\n","output_type":"stream"},{"name":"stderr","text":"Aug oil_spill: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 432/432 [00:01<00:00, 291.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸŸ¦ Saved balance report: /kaggle/working/run_min_balance/reports/balance_report.json\n\nðŸŸ¦ Saved class mapping: /kaggle/working/run_min_balance/models/class_mapping.json\n\nðŸŸ¦ Creating stratified splits...\n\nðŸŸ¦ Split sizes: train=8465 val=1814 test=1814\n\nðŸŸ¦ Exporting train split to: /kaggle/working/run_min_balance/splits/train\n","output_type":"stream"},{"name":"stderr","text":"Copying train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8465/8465 [00:01<00:00, 6158.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸŸ¦ Exporting val split to: /kaggle/working/run_min_balance/splits/val\n","output_type":"stream"},{"name":"stderr","text":"Copying val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1814/1814 [00:00<00:00, 6364.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸŸ¦ Exporting test split to: /kaggle/working/run_min_balance/splits/test\n","output_type":"stream"},{"name":"stderr","text":"Copying test: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1814/1814 [00:00<00:00, 6340.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nðŸŸ¦ Creating TEST.zip...\n\nðŸŸ¦ TEST.zip created: /kaggle/working/run_min_balance/TEST.zip (18.66 MB)\n\nðŸŸ¦ Building MobileNetV2 (small + stable)...\n\nðŸŸ¦ Training phase 1 (frozen backbone)...\nEpoch 1/20\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step - accuracy: 0.7180 - loss: 1.0487\nEpoch 1: val_loss improved from inf to 0.92775, saving model to /kaggle/working/run_min_balance/models/best.weights.h5\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 426ms/step - accuracy: 0.7181 - loss: 1.0485 - val_accuracy: 0.7233 - val_loss: 0.9277 - learning_rate: 3.0000e-04\nEpoch 2/20\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.7052 - loss: 1.0054\nEpoch 2: val_loss improved from 0.92775 to 0.78551, saving model to /kaggle/working/run_min_balance/models/best.weights.h5\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 207ms/step - accuracy: 0.7056 - loss: 1.0046 - val_accuracy: 0.7641 - val_loss: 0.7855 - learning_rate: 3.0000e-04\nEpoch 3/20\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.7487 - loss: 0.9003\nEpoch 3: val_loss improved from 0.78551 to 0.73568, saving model to /kaggle/working/run_min_balance/models/best.weights.h5\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 208ms/step - accuracy: 0.7490 - loss: 0.8997 - val_accuracy: 0.7938 - val_loss: 0.7357 - learning_rate: 3.0000e-04\nEpoch 4/20\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.7781 - loss: 0.8245\nEpoch 4: val_loss improved from 0.73568 to 0.71565, saving model to /kaggle/working/run_min_balance/models/best.weights.h5\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 207ms/step - accuracy: 0.7783 - loss: 0.8241 - val_accuracy: 0.8060 - val_loss: 0.7157 - learning_rate: 3.0000e-04\nEpoch 5/20\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8070 - loss: 0.7559\nEpoch 5: val_loss improved from 0.71565 to 0.69395, saving model to /kaggle/working/run_min_balance/models/best.weights.h5\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 207ms/step - accuracy: 0.8072 - loss: 0.7556 - val_accuracy: 0.8192 - val_loss: 0.6939 - learning_rate: 3.0000e-04\nEpoch 6/20\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8285 - loss: 0.7299\nEpoch 6: val_loss improved from 0.69395 to 0.67275, saving model to /kaggle/working/run_min_balance/models/best.weights.h5\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 206ms/step - accuracy: 0.8287 - loss: 0.7296 - val_accuracy: 0.8302 - val_loss: 0.6727 - learning_rate: 3.0000e-04\nEpoch 7/20\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8543 - loss: 0.6856\nEpoch 7: val_loss did not improve from 0.67275\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 205ms/step - accuracy: 0.8544 - loss: 0.6853 - val_accuracy: 0.8297 - val_loss: 0.6793 - learning_rate: 3.0000e-04\nEpoch 8/20\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8675 - loss: 0.6567\nEpoch 8: val_loss did not improve from 0.67275\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 205ms/step - accuracy: 0.8676 - loss: 0.6565 - val_accuracy: 0.8203 - val_loss: 0.6860 - learning_rate: 3.0000e-04\nEpoch 9/20\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8829 - loss: 0.6327\nEpoch 9: val_loss did not improve from 0.67275\n\nEpoch 9: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 205ms/step - accuracy: 0.8830 - loss: 0.6326 - val_accuracy: 0.8275 - val_loss: 0.6732 - learning_rate: 3.0000e-04\nEpoch 10/20\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.8496 - loss: 0.6855\nEpoch 10: val_loss improved from 0.67275 to 0.59110, saving model to /kaggle/working/run_min_balance/models/best.weights.h5\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 208ms/step - accuracy: 0.8498 - loss: 0.6853 - val_accuracy: 0.8749 - val_loss: 0.5911 - learning_rate: 1.5000e-04\nEpoch 11/20\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8921 - loss: 0.6032\nEpoch 11: val_loss improved from 0.59110 to 0.58405, saving model to /kaggle/working/run_min_balance/models/best.weights.h5\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 206ms/step - accuracy: 0.8921 - loss: 0.6032 - val_accuracy: 0.8765 - val_loss: 0.5841 - learning_rate: 1.5000e-04\nEpoch 12/20\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8952 - loss: 0.6029\nEpoch 12: val_loss did not improve from 0.58405\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 205ms/step - accuracy: 0.8953 - loss: 0.6028 - val_accuracy: 0.8738 - val_loss: 0.5870 - learning_rate: 1.5000e-04\nEpoch 13/20\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.8906 - loss: 0.6054\nEpoch 13: val_loss did not improve from 0.58405\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 204ms/step - accuracy: 0.8907 - loss: 0.6053 - val_accuracy: 0.8716 - val_loss: 0.5890 - learning_rate: 1.5000e-04\nEpoch 14/20\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.8973 - loss: 0.5951\nEpoch 14: val_loss improved from 0.58405 to 0.57718, saving model to /kaggle/working/run_min_balance/models/best.weights.h5\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 207ms/step - accuracy: 0.8973 - loss: 0.5950 - val_accuracy: 0.8798 - val_loss: 0.5772 - learning_rate: 1.5000e-04\nEpoch 15/20\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 202ms/step - accuracy: 0.9046 - loss: 0.5741\nEpoch 15: val_loss improved from 0.57718 to 0.57574, saving model to /kaggle/working/run_min_balance/models/best.weights.h5\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 207ms/step - accuracy: 0.9047 - loss: 0.5740 - val_accuracy: 0.8776 - val_loss: 0.5757 - learning_rate: 1.5000e-04\nEpoch 16/20\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.9084 - loss: 0.5801\nEpoch 16: val_loss improved from 0.57574 to 0.57427, saving model to /kaggle/working/run_min_balance/models/best.weights.h5\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 208ms/step - accuracy: 0.9085 - loss: 0.5800 - val_accuracy: 0.8793 - val_loss: 0.5743 - learning_rate: 1.5000e-04\nEpoch 17/20\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.9163 - loss: 0.5587\nEpoch 17: val_loss did not improve from 0.57427\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 204ms/step - accuracy: 0.9163 - loss: 0.5586 - val_accuracy: 0.8749 - val_loss: 0.5826 - learning_rate: 1.5000e-04\nEpoch 18/20\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.9096 - loss: 0.5755\nEpoch 18: val_loss did not improve from 0.57427\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 204ms/step - accuracy: 0.9097 - loss: 0.5754 - val_accuracy: 0.8809 - val_loss: 0.5767 - learning_rate: 1.5000e-04\nEpoch 19/20\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.9206 - loss: 0.5573\nEpoch 19: val_loss improved from 0.57427 to 0.56483, saving model to /kaggle/working/run_min_balance/models/best.weights.h5\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 206ms/step - accuracy: 0.9206 - loss: 0.5573 - val_accuracy: 0.8831 - val_loss: 0.5648 - learning_rate: 1.5000e-04\nEpoch 20/20\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 201ms/step - accuracy: 0.9197 - loss: 0.5459\nEpoch 20: val_loss improved from 0.56483 to 0.56186, saving model to /kaggle/working/run_min_balance/models/best.weights.h5\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 206ms/step - accuracy: 0.9197 - loss: 0.5459 - val_accuracy: 0.8886 - val_loss: 0.5619 - learning_rate: 1.5000e-04\n\nðŸŸ¦ Fine-tuning (unfreeze top 30%)...\nEpoch 1/8\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339ms/step - accuracy: 0.6941 - loss: 1.0635\nEpoch 1: val_loss did not improve from 0.56186\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 424ms/step - accuracy: 0.6944 - loss: 1.0628 - val_accuracy: 0.8754 - val_loss: 0.6243 - learning_rate: 1.0000e-05\nEpoch 2/8\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.7708 - loss: 0.8523\nEpoch 2: val_loss did not improve from 0.56186\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 208ms/step - accuracy: 0.7710 - loss: 0.8519 - val_accuracy: 0.8875 - val_loss: 0.5868 - learning_rate: 1.0000e-05\nEpoch 3/8\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.8175 - loss: 0.7534\nEpoch 3: val_loss did not improve from 0.56186\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 208ms/step - accuracy: 0.8176 - loss: 0.7532 - val_accuracy: 0.8969 - val_loss: 0.5735 - learning_rate: 1.0000e-05\nEpoch 4/8\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.8336 - loss: 0.7540\nEpoch 4: val_loss improved from 0.56186 to 0.56036, saving model to /kaggle/working/run_min_balance/models/best.weights.h5\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 209ms/step - accuracy: 0.8338 - loss: 0.7537 - val_accuracy: 0.9008 - val_loss: 0.5604 - learning_rate: 1.0000e-05\nEpoch 5/8\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.8400 - loss: 0.7310\nEpoch 5: val_loss improved from 0.56036 to 0.55433, saving model to /kaggle/working/run_min_balance/models/best.weights.h5\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 210ms/step - accuracy: 0.8401 - loss: 0.7306 - val_accuracy: 0.8964 - val_loss: 0.5543 - learning_rate: 1.0000e-05\nEpoch 6/8\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.8454 - loss: 0.7001\nEpoch 6: val_loss improved from 0.55433 to 0.54510, saving model to /kaggle/working/run_min_balance/models/best.weights.h5\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 209ms/step - accuracy: 0.8455 - loss: 0.6999 - val_accuracy: 0.9085 - val_loss: 0.5451 - learning_rate: 1.0000e-05\nEpoch 7/8\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.8720 - loss: 0.6659\nEpoch 7: val_loss improved from 0.54510 to 0.54077, saving model to /kaggle/working/run_min_balance/models/best.weights.h5\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 209ms/step - accuracy: 0.8721 - loss: 0.6657 - val_accuracy: 0.9101 - val_loss: 0.5408 - learning_rate: 1.0000e-05\nEpoch 8/8\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 203ms/step - accuracy: 0.8697 - loss: 0.6589\nEpoch 8: val_loss improved from 0.54077 to 0.53508, saving model to /kaggle/working/run_min_balance/models/best.weights.h5\n\u001b[1m265/265\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 209ms/step - accuracy: 0.8698 - loss: 0.6586 - val_accuracy: 0.9157 - val_loss: 0.5351 - learning_rate: 1.0000e-05\n\nðŸŸ¦ Saving history CSV...\n\nðŸŸ¦ Saving graphs...\n\nðŸŸ¦ Evaluating on test set...\n\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 214ms/step\n\nðŸŸ¦ Saving model...\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"INFO:tensorflow:Assets written to: /kaggle/working/run_min_balance/models/saved_model/assets\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:Assets written to: /kaggle/working/run_min_balance/models/saved_model/assets\n","output_type":"stream"},{"name":"stdout","text":"Saved artifact at '/kaggle/working/run_min_balance/models/saved_model'. The following endpoints are available:\n\n* Endpoint 'serve'\n  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor_518')\nOutput Type:\n  TensorSpec(shape=(None, 7), dtype=tf.float32, name=None)\nCaptures:\n  132417950570576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950573072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950573456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950579600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950573264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950574608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950573840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950573648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950572496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950575376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950574800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950574992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950575184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950569808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950576528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950575952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950576144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950576336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950579024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950575568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950576912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950577104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950577296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950574032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950578448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950577872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950578064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950578256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950575760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950580176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950579408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950576720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950579792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950579216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950579984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950580368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641071824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641072016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950577680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417950578832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641070672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641071440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641071632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641071248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641072976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641072400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641072592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641072784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641070864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641073936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641073360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641073552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641073744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641071056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641074896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641074320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641074512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641074704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641072208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641075856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641075280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641075472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641075664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641073168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641076816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641076240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641076432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641076624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641074128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641077776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641077200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641077392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641077584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641075088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641078736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641078160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641078352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641078544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641076048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641079696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420782744272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641080272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641079504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641077968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641080656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641080080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641080464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641077008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641079888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641081616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641081040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641081232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641081424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641078928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641082576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641082000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641082192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641082384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641079120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641083536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641082960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641083152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641083344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641080848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641084496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641083920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641084112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641084304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641081808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641085456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641084880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641085072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641085264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641082768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641086416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641085840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641083728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641086608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641086032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641086800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641086224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605305168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605304976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641084688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417641085648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605304400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605305552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605305360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605304592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605306704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605306128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605306320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605306512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605305744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605307664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605307088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605307280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605307472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605304784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605308624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605308048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605308240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605308432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605305936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605309584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605309008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605309200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605309392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605306896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605310544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605309968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605310160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605310352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605307856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605311504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605310928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605311120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605311312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605308816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605312464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605311888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605312080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605312272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605309776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605313424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605312848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605313040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605313232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605310736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605314384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605313808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605314000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605314192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605311696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605315344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605314768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605314960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605315152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605312656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605316304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605315728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605315920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605316112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605313616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605317264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605316688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605316880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605317072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605314576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605318224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605317648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605317840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605318032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605315536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605319184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605318608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605318800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605318992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605316496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605320144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605319568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605317456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605320336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605319760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605320528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605319952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474488016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474488208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605318416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417605319376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474486864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474487632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474487824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474487440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474489168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474488592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474488784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474488976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474487056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474490128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474489552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474489744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474489936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474487248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474491088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474490512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474490704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474490896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474488400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474492048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474491472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474491664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474491856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474489360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474493008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474492432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474492624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474492816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474490320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474493968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474493392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474493584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474493776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474491280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474494928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474494352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474494544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474494736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474492240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474495888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474495312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474495504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474495696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474493200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474496848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474496272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474496464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474496656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474494160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474497808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474496080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474495120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474497616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474498960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474498768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474497424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474499344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420474499152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n\nðŸŸ¦ âœ… DONE.\nArtifacts:\n - /kaggle/working/run_min_balance\n - Balanced dataset: /kaggle/working/run_min_balance/balanced_dataset\n - TEST.zip: /kaggle/working/run_min_balance/TEST.zip\n - Plots: /kaggle/working/run_min_balance/plots\n - Reports: /kaggle/working/run_min_balance/reports\n - Models: /kaggle/working/run_min_balance/models\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import os\nimport zipfile\nfrom pathlib import Path\n\nRUN_DIR = Path(\"/kaggle/working/run_min_balance\")\nOUT_ZIP = RUN_DIR / \"deploy_bundle.zip\"\n\nINCLUDE = [\n    RUN_DIR / \"models\" / \"model_best.keras\",\n    RUN_DIR / \"models\" / \"class_mapping.json\",\n    RUN_DIR / \"TEST.zip\",\n    RUN_DIR / \"plots\" / \"confusion_matrix.png\",\n    RUN_DIR / \"reports\" / \"classification_report.json\",\n    RUN_DIR / \"reports\" / \"confusion_matrix.json\",\n    RUN_DIR / \"models\" / \"saved_model\",   # folder\n]\n\ndef add_path(zf: zipfile.ZipFile, p: Path, arc_base: Path):\n    if p.is_file():\n        zf.write(p, arcname=str(p.relative_to(arc_base)))\n    elif p.is_dir():\n        for fp in p.rglob(\"*\"):\n            if fp.is_file():\n                zf.write(fp, arcname=str(fp.relative_to(arc_base)))\n\nif OUT_ZIP.exists():\n    OUT_ZIP.unlink()\n\nwith zipfile.ZipFile(OUT_ZIP, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n    for p in INCLUDE:\n        if not p.exists():\n            print(f\"âš ï¸ Missing, skipping: {p}\")\n            continue\n        add_path(zf, p, RUN_DIR)\n\nprint(\"âœ… Created:\", OUT_ZIP)\nprint(\"ðŸ“¦ Size (MB):\", round(OUT_ZIP.stat().st_size / (1024*1024), 2))\nprint(\"âž¡ï¸ Download it from Kaggle output panel: /kaggle/working/run_min_balance/deploy_bundle.zip\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T18:19:13.369425Z","iopub.execute_input":"2026-02-03T18:19:13.369780Z","iopub.status.idle":"2026-02-03T18:19:16.036014Z","shell.execute_reply.started":"2026-02-03T18:19:13.369752Z","shell.execute_reply":"2026-02-03T18:19:16.035371Z"}},"outputs":[{"name":"stdout","text":"âœ… Created: /kaggle/working/run_min_balance/deploy_bundle.zip\nðŸ“¦ Size (MB): 52.82\nâž¡ï¸ Download it from Kaggle output panel: /kaggle/working/run_min_balance/deploy_bundle.zip\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import json, random\nfrom pathlib import Path\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n# Paths\nRUN_DIR = Path(\"/kaggle/working/run_min_balance\")\nMODEL_PATH = RUN_DIR / \"models\" / \"model_best.keras\"\nMAP_PATH = RUN_DIR / \"models\" / \"class_mapping.json\"\nTEST_DIR = RUN_DIR / \"splits\" / \"test\"\n\nassert MODEL_PATH.exists(), f\"Missing model: {MODEL_PATH}\"\nassert MAP_PATH.exists(), f\"Missing class_mapping: {MAP_PATH}\"\nassert TEST_DIR.exists(), f\"Missing test folder: {TEST_DIR}\"\n\n# Load mapping + model\nmapping = json.loads(MAP_PATH.read_text())\nclasses = mapping[\"classes\"]\nclass_to_idx = mapping[\"class_to_idx\"]\nidx_to_class = {v: k for k, v in class_to_idx.items()}\n\nprint(\"âœ… Classes:\", classes)\n\nmodel = tf.keras.models.load_model(MODEL_PATH)\nprint(\"âœ… Model loaded:\", MODEL_PATH)\n\nIMG_SIZE = 224\n\ndef list_images(folder: Path):\n    exts = {\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\", \".tif\", \".tiff\"}\n    return [p for p in folder.rglob(\"*\") if p.is_file() and p.suffix.lower() in exts]\n\n# Build test file list\ntest_files, test_labels = [], []\nfor cls in classes:\n    folder = TEST_DIR / cls\n    if not folder.exists():\n        continue\n    imgs = list_images(folder)\n    test_files.extend([str(p) for p in imgs])\n    test_labels.extend([cls] * len(imgs))\n\ny_true = np.array([class_to_idx[c] for c in test_labels], dtype=np.int32)\nprint(f\"âœ… Test samples: {len(test_files)}\")\n\n# tf.data for test\ndef make_test_ds(paths):\n    ds = tf.data.Dataset.from_tensor_slices(paths)\n\n    def _load(path):\n        img = tf.io.read_file(path)\n        img = tf.image.decode_jpeg(img, channels=3)\n        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n        img = tf.cast(img, tf.float32)\n        return img\n\n    ds = ds.map(_load, num_parallel_calls=tf.data.AUTOTUNE).batch(32).prefetch(tf.data.AUTOTUNE)\n    return ds\n\ntest_ds = make_test_ds(test_files)\n\n# Predict\nprobs = model.predict(test_ds, verbose=1)\ny_pred = np.argmax(probs, axis=1)\n\nacc = float(np.mean(y_pred == y_true))\nprint(\"\\nâœ… Overall Test Accuracy:\", round(acc, 4))\n\n# Confusion matrix + report\ncm = confusion_matrix(y_true, y_pred)\nprint(\"\\nConfusion Matrix:\\n\", cm)\n\nprint(\"\\nClassification Report:\\n\")\nprint(classification_report(y_true, y_pred, target_names=classes, zero_division=0))\n\n# Per-class accuracy\nprint(\"\\nPer-class accuracy:\")\nfor i, cls in enumerate(classes):\n    denom = cm[i].sum()\n    per_acc = (cm[i, i] / denom) if denom > 0 else 0.0\n    print(f\"  - {cls:12s}: {per_acc:.3f} ({cm[i,i]}/{denom})\")\n\n# Manual random samples\nprint(\"\\nðŸ”Ž Random sample predictions (from TEST split):\")\nsample_n = 12\nidxs = random.sample(range(len(test_files)), min(sample_n, len(test_files)))\n\nfor k in idxs:\n    path = test_files[k]\n    true_cls = test_labels[k]\n    p = probs[k]\n    pred_idx = int(np.argmax(p))\n    pred_cls = idx_to_class[pred_idx]\n    conf = float(np.max(p))\n    print(f\"- True={true_cls:12s}  Pred={pred_cls:12s}  Conf={conf:.3f}  File={Path(path).name}\")\n\nprint(\"\\nâœ… Done.\")\nprint(\"If you want to test a specific image, paste its path and run the helper below.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T18:23:25.604329Z","iopub.execute_input":"2026-02-03T18:23:25.605030Z","iopub.status.idle":"2026-02-03T18:23:49.948457Z","shell.execute_reply.started":"2026-02-03T18:23:25.605000Z","shell.execute_reply":"2026-02-03T18:23:49.947770Z"}},"outputs":[{"name":"stdout","text":"âœ… Classes: ['cardboard', 'glass', 'marine_trash', 'metal', 'oil_spill', 'paper', 'plastic']\nâœ… Model loaded: /kaggle/working/run_min_balance/models/model_best.keras\nâœ… Test samples: 1814\n\u001b[1m57/57\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 213ms/step\n\nâœ… Overall Test Accuracy: 0.9184\n\nConfusion Matrix:\n [[190   1   0   9   0  28   2]\n [  0 252   0  18   1   0  29]\n [  0   0 460   0   1   0   0]\n [  1   0   0 172   0   0   7]\n [  0   0   0   1 179   0   0]\n [  4   4   0   9   2 180   8]\n [  3  10   0   5   0   5 233]]\n\nClassification Report:\n\n              precision    recall  f1-score   support\n\n   cardboard       0.96      0.83      0.89       230\n       glass       0.94      0.84      0.89       300\nmarine_trash       1.00      1.00      1.00       461\n       metal       0.80      0.96      0.87       180\n   oil_spill       0.98      0.99      0.99       180\n       paper       0.85      0.87      0.86       207\n     plastic       0.84      0.91      0.87       256\n\n    accuracy                           0.92      1814\n   macro avg       0.91      0.91      0.91      1814\nweighted avg       0.92      0.92      0.92      1814\n\n\nPer-class accuracy:\n  - cardboard   : 0.826 (190/230)\n  - glass       : 0.840 (252/300)\n  - marine_trash: 0.998 (460/461)\n  - metal       : 0.956 (172/180)\n  - oil_spill   : 0.994 (179/180)\n  - paper       : 0.870 (180/207)\n  - plastic     : 0.910 (233/256)\n\nðŸ”Ž Random sample predictions (from TEST split):\n- True=oil_spill     Pred=oil_spill     Conf=0.996  File=oil_spill_0000489.jpg\n- True=cardboard     Pred=cardboard     Conf=0.923  File=cardboard_0000316.jpg\n- True=marine_trash  Pred=marine_trash  Conf=0.982  File=marine_trash_0002421.jpg\n- True=marine_trash  Pred=marine_trash  Conf=0.989  File=marine_trash_0000769.jpg\n- True=oil_spill     Pred=oil_spill     Conf=0.978  File=oil_spill_0000377.jpg\n- True=paper         Pred=paper         Conf=0.572  File=paper_0001317.jpg\n- True=plastic       Pred=plastic       Conf=0.937  File=plastic_0000180.jpg\n- True=marine_trash  Pred=marine_trash  Conf=0.984  File=marine_trash_0001059.jpg\n- True=marine_trash  Pred=marine_trash  Conf=0.989  File=marine_trash_0002217.jpg\n- True=marine_trash  Pred=marine_trash  Conf=0.993  File=marine_trash_0000184.jpg\n- True=marine_trash  Pred=marine_trash  Conf=0.988  File=marine_trash_0000281.jpg\n- True=plastic       Pred=plastic       Conf=0.876  File=plastic_0000832.jpg\n\nâœ… Done.\nIf you want to test a specific image, paste its path and run the helper below.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from pathlib import Path\n\nDATA_DIR = Path(\"/kaggle/input/processed-again-costal-polutant\")\n\ncandidates = [\n    DATA_DIR \"/kaggle/input/procesed-again-costal-polutant/processed_marine_waste/clean_underwater\",\n    DATA_DIR / \"processed_marine_waste\" / \"clean underwater\",\n    DATA_DIR / \"processed_marine_waste\" / \"clean\",\n    DATA_DIR / \"processed_oilspill\" / \"no_oil_spill\",\n    DATA_DIR / \"processed_oilspill\" / \"no oil spill\",\n    DATA_DIR / \"processed_oilspill\" / \"no_oil\",\n]\n\ndef list_images(folder: Path):\n    exts = {\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\", \".tif\", \".tiff\"}\n    return [p for p in folder.rglob(\"*\") if p.is_file() and p.suffix.lower() in exts]\n\nprint(\"Checking candidates:\")\nfor p in candidates:\n    if p.exists():\n        imgs = list_images(p)\n        print(f\"âœ… {p}  -> {len(imgs)} images\")\n    else:\n        print(f\"âŒ {p}  (missing)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T18:26:39.222442Z","iopub.execute_input":"2026-02-03T18:26:39.223185Z","iopub.status.idle":"2026-02-03T18:26:39.229846Z","shell.execute_reply.started":"2026-02-03T18:26:39.223153Z","shell.execute_reply":"2026-02-03T18:26:39.228962Z"}},"outputs":[{"name":"stdout","text":"Checking candidates:\nâŒ /kaggle/input/processed-again-costal-polutant/processed_marine_waste/clean_underwater  (missing)\nâŒ /kaggle/input/processed-again-costal-polutant/processed_marine_waste/clean underwater  (missing)\nâŒ /kaggle/input/processed-again-costal-polutant/processed_marine_waste/clean  (missing)\nâŒ /kaggle/input/processed-again-costal-polutant/processed_oilspill/no_oil_spill  (missing)\nâŒ /kaggle/input/processed-again-costal-polutant/processed_oilspill/no oil spill  (missing)\nâŒ /kaggle/input/processed-again-costal-polutant/processed_oilspill/no_oil  (missing)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import json\nimport numpy as np\nimport tensorflow as tf\nfrom pathlib import Path\n\nRUN_DIR = Path(\"/kaggle/working/run_min_balance\")\nMODEL_PATH = RUN_DIR / \"models\" / \"model_best.keras\"\nMAP_PATH = RUN_DIR / \"models\" / \"class_mapping.json\"\n\nDATA_DIR = Path(\"/kaggle/input/processed-again-costal-polutant\")\n\nNEGATIVE_SOURCES = {\n    \"clean_underwater\": DATA_DIR / \"processed_marine_waste\" / \"clean_underwater\",\n    \"no_oil_spill\": DATA_DIR / \"processed_oilspill\" / \"no_oil_spill\",\n}\n\nmapping = json.loads(MAP_PATH.read_text())\nclasses = mapping[\"classes\"]\nidx_to_class = {v: k for k, v in mapping[\"class_to_idx\"].items()}\n\nmodel = tf.keras.models.load_model(MODEL_PATH)\n\nIMG_SIZE = 224\n\ndef list_images(folder: Path):\n    exts = {\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\", \".tif\", \".tiff\"}\n    return [p for p in folder.rglob(\"*\") if p.is_file() and p.suffix.lower() in exts]\n\nneg_files = []\nneg_names = []\nfor name, folder in NEGATIVE_SOURCES.items():\n    if folder.exists():\n        imgs = list_images(folder)\n        neg_files.extend([str(x) for x in imgs])\n        neg_names.extend([name]*len(imgs))\n\nprint(\"Negative samples:\", len(neg_files))\n\nif len(neg_files) == 0:\n    print(\"ðŸŸ¨ No negative images found. Either the folders are missing/empty.\")\n    print(\"ðŸ‘‰ Use a manual threshold like 0.60 for UNKNOWN gating, or add clean-water images.\")\nelse:\n    # Force dtype string explicitly\n    paths = tf.constant(neg_files, dtype=tf.string)\n\n    def _load(p):\n        img = tf.io.read_file(p)\n        # decode_image supports jpg/png safely\n        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n        img = tf.cast(img, tf.float32)\n        return img\n\n    ds = tf.data.Dataset.from_tensor_slices(paths).map(_load, num_parallel_calls=tf.data.AUTOTUNE).batch(32).prefetch(tf.data.AUTOTUNE)\n\n    probs = model.predict(ds, verbose=1)\n    maxp = np.max(probs, axis=1)\n    pred = np.argmax(probs, axis=1)\n    pred_cls = np.array([idx_to_class[int(i)] for i in pred])\n\n    thresholds = [0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70]\n    print(\"\\nThreshold test (how many negatives still get flagged as a class):\")\n    for t in thresholds:\n        flagged = np.mean(maxp >= t)\n        print(f\"  t={t:.2f} -> flagged negatives: {flagged*100:.2f}%\")\n\n    suggest = None\n    for t in thresholds:\n        if np.mean(maxp >= t) <= 0.05:\n            suggest = t\n            break\n    print(\"\\nSuggested threshold:\", suggest)\n\n    topk = np.argsort(-maxp)[:15]\n    print(\"\\nTop confident negative predictions (worst-case clean images):\")\n    for i in topk:\n        print(f\"  maxp={maxp[i]:.3f} pred={pred_cls[i]:12s} source={neg_names[i]} file={Path(neg_files[i]).name}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T18:29:16.537847Z","iopub.execute_input":"2026-02-03T18:29:16.538493Z","iopub.status.idle":"2026-02-03T18:29:17.686237Z","shell.execute_reply.started":"2026-02-03T18:29:16.538463Z","shell.execute_reply":"2026-02-03T18:29:17.685555Z"}},"outputs":[{"name":"stdout","text":"Negative samples: 0\nðŸŸ¨ No negative images found. Either the folders are missing/empty.\nðŸ‘‰ Use a manual threshold like 0.60 for UNKNOWN gating, or add clean-water images.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import json\nimport numpy as np\nimport tensorflow as tf\nfrom pathlib import Path\n\nRUN_DIR = Path(\"/kaggle/working/run_min_balance\")\nMODEL_PATH = RUN_DIR / \"models\" / \"model_best.keras\"\nMAP_PATH = RUN_DIR / \"models\" / \"class_mapping.json\"\n\n# âœ… Correct dataset root (your exact path)\nDATA_DIR = Path(\"/kaggle/input/procesed-again-costal-polutant\")\n\nNEGATIVE_SOURCES = {\n    \"clean_underwater\": DATA_DIR / \"processed_marine_waste\" / \"clean_underwater\",\n    \"no_oil_spill\": DATA_DIR / \"processed_oilspill\" / \"no_oil_spill\",\n}\n\nmapping = json.loads(MAP_PATH.read_text())\nidx_to_class = {v: k for k, v in mapping[\"class_to_idx\"].items()}\n\nmodel = tf.keras.models.load_model(MODEL_PATH)\n\nIMG_SIZE = 224\n\ndef list_images(folder: Path):\n    exts = {\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\", \".tif\", \".tiff\"}\n    return [p for p in folder.rglob(\"*\") if p.is_file() and p.suffix.lower() in exts]\n\n# Collect negative files\nneg_files = []\nneg_names = []\nfor name, folder in NEGATIVE_SOURCES.items():\n    print(\"Checking:\", folder, \"exists=\", folder.exists())\n    if folder.exists():\n        imgs = list_images(folder)\n        print(f\"  -> found {len(imgs)} images\")\n        neg_files.extend([str(x) for x in imgs])\n        neg_names.extend([name]*len(imgs))\n\nprint(\"\\nNegative samples total:\", len(neg_files))\n\nif len(neg_files) == 0:\n    print(\"ðŸŸ¨ Still zero negatives. Double-check the folder has images.\")\nelse:\n    # Force dtype string\n    paths = tf.constant(neg_files, dtype=tf.string)\n\n    def _load(p):\n        img = tf.io.read_file(p)\n        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n        img = tf.cast(img, tf.float32)\n        return img\n\n    ds = tf.data.Dataset.from_tensor_slices(paths)\\\n        .map(_load, num_parallel_calls=tf.data.AUTOTUNE)\\\n        .batch(32)\\\n        .prefetch(tf.data.AUTOTUNE)\n\n    probs = model.predict(ds, verbose=1)\n    maxp = np.max(probs, axis=1)\n    pred = np.argmax(probs, axis=1)\n    pred_cls = np.array([idx_to_class[int(i)] for i in pred])\n\n    thresholds = [0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70, 0.75]\n    print(\"\\nThreshold test (negatives flagged as some class):\")\n    for t in thresholds:\n        flagged = np.mean(maxp >= t)\n        print(f\"  t={t:.2f} -> flagged negatives: {flagged*100:.2f}%\")\n\n    suggest = None\n    for t in thresholds:\n        if np.mean(maxp >= t) <= 0.05:\n            suggest = t\n            break\n    print(\"\\nâœ… Suggested unknown threshold (<=5% false alarms):\", suggest)\n\n    topk = np.argsort(-maxp)[:15]\n    print(\"\\nTop confident NEGATIVE predictions (worst-case clean images):\")\n    for i in topk:\n        print(f\"  maxp={maxp[i]:.3f} pred={pred_cls[i]:12s} source={neg_names[i]} file={Path(neg_files[i]).name}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T18:30:55.203935Z","iopub.execute_input":"2026-02-03T18:30:55.204492Z","iopub.status.idle":"2026-02-03T18:31:33.454866Z","shell.execute_reply.started":"2026-02-03T18:30:55.204462Z","shell.execute_reply":"2026-02-03T18:31:33.454138Z"}},"outputs":[{"name":"stdout","text":"Checking: /kaggle/input/procesed-again-costal-polutant/processed_marine_waste/clean_underwater exists= True\n  -> found 1 images\nChecking: /kaggle/input/procesed-again-costal-polutant/processed_oilspill/no_oil_spill exists= True\n  -> found 412 images\n\nNegative samples total: 413\n\u001b[1m13/13\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2s/step\n\nThreshold test (negatives flagged as some class):\n  t=0.40 -> flagged negatives: 98.55%\n  t=0.45 -> flagged negatives: 97.82%\n  t=0.50 -> flagged negatives: 96.85%\n  t=0.55 -> flagged negatives: 96.61%\n  t=0.60 -> flagged negatives: 95.88%\n  t=0.65 -> flagged negatives: 93.95%\n  t=0.70 -> flagged negatives: 93.46%\n  t=0.75 -> flagged negatives: 92.49%\n\nâœ… Suggested unknown threshold (<=5% false alarms): None\n\nTop confident NEGATIVE predictions (worst-case clean images):\n  maxp=1.000 pred=oil_spill    source=no_oil_spill file=no_oil_0000145.jpg\n  maxp=1.000 pred=oil_spill    source=no_oil_spill file=no_oil_0000545.jpg\n  maxp=1.000 pred=oil_spill    source=no_oil_spill file=no_oil_0000581.jpg\n  maxp=1.000 pred=oil_spill    source=no_oil_spill file=no_oil_0000105.jpg\n  maxp=1.000 pred=oil_spill    source=no_oil_spill file=no_oil_0000023.jpg\n  maxp=1.000 pred=oil_spill    source=no_oil_spill file=no_oil_0000173.jpg\n  maxp=1.000 pred=oil_spill    source=no_oil_spill file=no_oil_0000048.jpg\n  maxp=1.000 pred=oil_spill    source=no_oil_spill file=no_oil_0000956.jpg\n  maxp=1.000 pred=oil_spill    source=no_oil_spill file=no_oil_0000316.jpg\n  maxp=1.000 pred=oil_spill    source=no_oil_spill file=no_oil_0000652.jpg\n  maxp=1.000 pred=oil_spill    source=no_oil_spill file=no_oil_0000129.jpg\n  maxp=1.000 pred=oil_spill    source=no_oil_spill file=no_oil_0000921.jpg\n  maxp=1.000 pred=oil_spill    source=no_oil_spill file=no_oil_0000008.jpg\n  maxp=1.000 pred=oil_spill    source=no_oil_spill file=no_oil_0000588.jpg\n  maxp=1.000 pred=oil_spill    source=no_oil_spill file=no_oil_0000150.jpg\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import json, shutil, zipfile\nfrom pathlib import Path\nimport numpy as np\nimport tensorflow as tf\nfrom tqdm import tqdm\n\n# ===== Paths =====\nRUN_DIR = Path(\"/kaggle/working/run_min_balance\")\nMODEL_PATH = RUN_DIR / \"models\" / \"model_best.keras\"\nMAP_PATH = RUN_DIR / \"models\" / \"class_mapping.json\"\n\nDATA_DIR = Path(\"/kaggle/input/procesed-again-costal-polutant\")\nNO_OIL_DIR = DATA_DIR / \"processed_oilspill\" / \"no_oil_spill\"\n\nOUT_ROOT = Path(\"/kaggle/working/clean_water_filtered\")\nOUT_DIR = OUT_ROOT / \"clean_water\"\nOUT_ZIP = Path(\"/kaggle/working/clean_water.zip\")\n\n# ===== Filters (tune these) =====\n# 1) If model thinks it's oil_spill with high confidence -> reject as \"not clean\"\nOIL_REJECT_CONF = 0.60\n\n# 2) If model is very confident it's some trash class -> reject (not clean)\nMAX_CONF_REJECT = 0.90\n\n# 3) If model is uncertain overall -> accept as clean-ish\nMAX_CONF_ACCEPT = 0.75\n\nIMG_SIZE = 224\n\ndef list_images(folder: Path):\n    exts = {\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\", \".tif\", \".tiff\"}\n    return [p for p in folder.rglob(\"*\") if p.is_file() and p.suffix.lower() in exts]\n\ndef make_ds(paths):\n    paths = tf.constant([str(p) for p in paths], dtype=tf.string)\n    ds = tf.data.Dataset.from_tensor_slices(paths)\n\n    def _load(p):\n        img = tf.io.read_file(p)\n        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n        img = tf.cast(img, tf.float32)\n        return img, p\n\n    return ds.map(_load, num_parallel_calls=tf.data.AUTOTUNE).batch(32).prefetch(tf.data.AUTOTUNE)\n\n# ===== Load model + mapping =====\nmapping = json.loads(MAP_PATH.read_text())\nclasses = mapping[\"classes\"]\nclass_to_idx = mapping[\"class_to_idx\"]\nidx_to_class = {v: k for k, v in class_to_idx.items()}\noil_idx = class_to_idx.get(\"oil_spill\", None)\n\nassert MODEL_PATH.exists(), f\"Missing model: {MODEL_PATH}\"\nassert MAP_PATH.exists(), f\"Missing mapping: {MAP_PATH}\"\nassert NO_OIL_DIR.exists(), f\"Missing folder: {NO_OIL_DIR}\"\nassert oil_idx is not None, \"oil_spill class not found in mapping!\"\n\nmodel = tf.keras.models.load_model(MODEL_PATH)\nprint(\"âœ… Loaded model + mapping\")\n\n# ===== Collect files =====\nfiles = list_images(NO_OIL_DIR)\nprint(\"âœ… no_oil_spill images:\", len(files))\n\nif OUT_ROOT.exists():\n    shutil.rmtree(OUT_ROOT, ignore_errors=True)\nOUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# ===== Predict + filter =====\nds = make_ds(files)\n\nkept = 0\nrejected_oil = 0\nrejected_confident = 0\n\nall_paths = []\nall_probs = []\n\n# collect predictions\nfor batch_imgs, batch_paths in tqdm(ds, desc=\"Predicting\"):\n    probs = model.predict(batch_imgs, verbose=0)\n    all_probs.append(probs)\n    all_paths.extend(batch_paths.numpy().tolist())\n\nall_probs = np.concatenate(all_probs, axis=0)\n\nfor i, pbytes in enumerate(all_paths):\n    p = all_probs[i]\n    maxp = float(np.max(p))\n    pred_idx = int(np.argmax(p))\n    pred_cls = idx_to_class[pred_idx]\n    oilp = float(p[oil_idx])\n\n    src_path = Path(pbytes.decode(\"utf-8\"))\n\n    # Reject if oil probability is high\n    if oilp >= OIL_REJECT_CONF:\n        rejected_oil += 1\n        continue\n\n    # Reject if model is extremely confident about any class (likely not clean)\n    if maxp >= MAX_CONF_REJECT:\n        rejected_confident += 1\n        continue\n\n    # Accept if model is overall uncertain (clean-ish)\n    if maxp <= MAX_CONF_ACCEPT:\n        dst = OUT_DIR / src_path.name\n        shutil.copy2(src_path, dst)\n        kept += 1\n\nprint(\"\\nâœ… Filtering done!\")\nprint(\"Kept (clean-ish):\", kept)\nprint(\"Rejected (oil-like):\", rejected_oil)\nprint(\"Rejected (too confident):\", rejected_confident)\nprint(\"Output folder:\", OUT_DIR)\n\n# ===== Zip for download =====\nif OUT_ZIP.exists():\n    OUT_ZIP.unlink()\n\nwith zipfile.ZipFile(OUT_ZIP, \"w\", zipfile.ZIP_DEFLATED) as zf:\n    for fp in OUT_ROOT.rglob(\"*\"):\n        if fp.is_file():\n            zf.write(fp, arcname=str(fp.relative_to(OUT_ROOT)))\n\nprint(\"\\nðŸ“¦ Created:\", OUT_ZIP)\nprint(\"Size (MB):\", round(OUT_ZIP.stat().st_size / (1024*1024), 2))\nprint(\"âž¡ï¸ Download: /kaggle/working/clean_water.zip\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T18:38:23.325601Z","iopub.execute_input":"2026-02-03T18:38:23.326088Z","iopub.status.idle":"2026-02-03T18:39:01.705067Z","shell.execute_reply.started":"2026-02-03T18:38:23.326056Z","shell.execute_reply":"2026-02-03T18:39:01.704275Z"}},"outputs":[{"name":"stdout","text":"âœ… Loaded model + mapping\nâœ… no_oil_spill images: 412\n","output_type":"stream"},{"name":"stderr","text":"Predicting: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:36<00:00,  2.84s/it]","output_type":"stream"},{"name":"stdout","text":"\nâœ… Filtering done!\nKept (clean-ish): 27\nRejected (oil-like): 385\nRejected (too confident): 0\nOutput folder: /kaggle/working/clean_water_filtered/clean_water\n\nðŸ“¦ Created: /kaggle/working/clean_water.zip\nSize (MB): 0.42\nâž¡ï¸ Download: /kaggle/working/clean_water.zip\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"arnaud58/landscape-pictures\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T18:48:48.018796Z","iopub.execute_input":"2026-02-03T18:48:48.019097Z","iopub.status.idle":"2026-02-03T18:49:11.394276Z","shell.execute_reply.started":"2026-02-03T18:48:48.019074Z","shell.execute_reply":"2026-02-03T18:49:11.393542Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/landscape-pictures\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import shutil, zipfile\nfrom pathlib import Path\nfrom PIL import Image, ImageFile\nfrom tqdm import tqdm\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n# =========================\n# CONFIG\n# =========================\nIMG_SIZE = 224\nMAX_IMAGES = 2500  # cap so clean_water doesn't dominate; adjust if you want\n\nLANDSCAPE_12K = Path(\"/kaggle/input/landscape-recognition-image-dataset-12k-images\")\n\nOUT_ROOT = Path(\"/kaggle/working/clean_water_dataset\")\nOUT_CLASS_DIR = OUT_ROOT / \"clean_water\"\nOUT_ZIP = Path(\"/kaggle/working/clean_water.zip\")\n\nMERGE_TO_EXISTING = True\nMERGE_TARGET = Path(\"/kaggle/working/run_min_balance/balanced_dataset\")  # your working dataset\n\n# =========================\n# Helpers\n# =========================\ndef list_images(folder: Path):\n    exts = {\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\", \".tif\", \".tiff\"}\n    return [p for p in folder.rglob(\"*\") if p.is_file() and p.suffix.lower() in exts]\n\ndef preprocess_copy(img_paths, out_dir: Path, prefix: str):\n    out_dir.mkdir(parents=True, exist_ok=True)\n    kept = 0\n    for fp in tqdm(img_paths, desc=f\"Preprocess {prefix}\"):\n        try:\n            im = Image.open(fp).convert(\"RGB\")\n            im = im.resize((IMG_SIZE, IMG_SIZE), Image.BILINEAR)\n            im.save(out_dir / f\"{prefix}_{kept:07d}.jpg\", quality=90, optimize=True)\n            kept += 1\n        except Exception:\n            continue\n    return kept\n\ndef make_zip(folder: Path, zip_path: Path):\n    if zip_path.exists():\n        zip_path.unlink()\n    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n        for fp in folder.rglob(\"*\"):\n            if fp.is_file():\n                zf.write(fp, arcname=str(fp.relative_to(folder)))\n\n# =========================\n# Coast dirs (exact structure you showed)\n# =========================\nbase = LANDSCAPE_12K / \"Landscape Classification\" / \"Landscape Classification\"\n\ntrain_coast = base / \"Training Data\" / \"Coast\"\nval_coast   = base / \"Validation Data\" / \"Coast\"\ntest_coast  = base / \"Testing Data\" / \"Coast\"\n\nprint(\"Coast train:\", train_coast, \"exists=\", train_coast.exists())\nprint(\"Coast val  :\", val_coast, \"exists=\", val_coast.exists())\nprint(\"Coast test :\", test_coast, \"exists=\", test_coast.exists())\n\ncoast_imgs = []\nfor p in [train_coast, val_coast, test_coast]:\n    coast_imgs += list_images(p)\n\nprint(\"\\nâœ… Total coast images found:\", len(coast_imgs))\n\n# cap\nif MAX_IMAGES is not None and len(coast_imgs) > MAX_IMAGES:\n    coast_imgs = coast_imgs[:MAX_IMAGES]\n    print(\"ðŸ§¢ Capped to:\", len(coast_imgs))\n\n# =========================\n# Preprocess output\n# =========================\nif OUT_ROOT.exists():\n    shutil.rmtree(OUT_ROOT, ignore_errors=True)\nOUT_CLASS_DIR.mkdir(parents=True, exist_ok=True)\n\nk = preprocess_copy(coast_imgs, OUT_CLASS_DIR, \"coast\")\n\nprint(\"\\nâœ… Saved clean_water images:\", k)\nprint(\"Output folder:\", OUT_CLASS_DIR)\n\n# =========================\n# Zip\n# =========================\nmake_zip(OUT_ROOT, OUT_ZIP)\nprint(\"\\nðŸ“¦ Created zip:\", OUT_ZIP)\nprint(\"âž¡ï¸ Download: /kaggle/working/clean_water.zip\")\n\n# =========================\n# Merge into dataset\n# =========================\nif MERGE_TO_EXISTING:\n    target = MERGE_TARGET / \"clean_water\"\n    target.mkdir(parents=True, exist_ok=True)\n    for fp in OUT_CLASS_DIR.glob(\"*.jpg\"):\n        shutil.copy2(fp, target / fp.name)\n    print(\"\\nâœ… Merged into:\", target)\n    print(\"Merged count:\", len(list_images(target)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T18:58:28.404104Z","iopub.execute_input":"2026-02-03T18:58:28.404389Z","iopub.status.idle":"2026-02-03T18:58:45.137260Z","shell.execute_reply.started":"2026-02-03T18:58:28.404364Z","shell.execute_reply":"2026-02-03T18:58:45.136504Z"}},"outputs":[{"name":"stdout","text":"Coast train: /kaggle/input/landscape-recognition-image-dataset-12k-images/Landscape Classification/Landscape Classification/Training Data/Coast exists= True\nCoast val  : /kaggle/input/landscape-recognition-image-dataset-12k-images/Landscape Classification/Landscape Classification/Validation Data/Coast exists= True\nCoast test : /kaggle/input/landscape-recognition-image-dataset-12k-images/Landscape Classification/Landscape Classification/Testing Data/Coast exists= True\n\nâœ… Total coast images found: 2400\n","output_type":"stream"},{"name":"stderr","text":"Preprocess coast: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2400/2400 [00:11<00:00, 202.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nâœ… Saved clean_water images: 2400\nOutput folder: /kaggle/working/clean_water_dataset/clean_water\n\nðŸ“¦ Created zip: /kaggle/working/clean_water.zip\nâž¡ï¸ Download: /kaggle/working/clean_water.zip\n\nâœ… Merged into: /kaggle/working/run_min_balance/balanced_dataset/clean_water\nMerged count: 2400\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import os, json, random, shutil, zipfile\nfrom pathlib import Path\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\n\n# =========================\n# CONFIG\n# =========================\nCONFIG = {\n    \"seed\": 42,\n    \"img_size\": 224,\n    \"batch_size\": 32,\n    \"epochs_phase1\": 10,\n    \"epochs_phase2\": 15,\n    \"lr_phase1\": 1e-3,\n    \"lr_phase2\": 1e-5,\n    \"dropout\": 0.4,\n    \"label_smoothing\": 0.05,\n    \"unfreeze_top_percent\": 0.30,\n    \"val_split\": 0.15,\n    \"test_split\": 0.15,\n\n    # âœ… Safety cap: prevents clean_water or any class from dominating\n    # Set to None to disable\n    \"max_per_class\": 1200,\n}\n\nDATASET_DIR = Path(\"/kaggle/working/run_min_balance/balanced_dataset\")\nOUT_DIR = Path(\"/kaggle/working/run_final_train\")\nMODEL_DIR = OUT_DIR / \"models\"\nPLOTS_DIR = OUT_DIR / \"plots\"\nREPORTS_DIR = OUT_DIR / \"reports\"\nSPLITS_DIR = OUT_DIR / \"splits\"\nTEST_ZIP = OUT_DIR / \"TEST.zip\"\n\nfor d in [MODEL_DIR, PLOTS_DIR, REPORTS_DIR, SPLITS_DIR]:\n    d.mkdir(parents=True, exist_ok=True)\n\nrandom.seed(CONFIG[\"seed\"])\nnp.random.seed(CONFIG[\"seed\"])\ntf.random.set_seed(CONFIG[\"seed\"])\n\n# =========================\n# GPU + mixed precision (safe)\n# =========================\ngpus = tf.config.list_physical_devices(\"GPU\")\nprint(\"GPUs:\", gpus)\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        from tensorflow.keras import mixed_precision\n        mixed_precision.set_global_policy(\"mixed_float16\")\n        print(\"âœ… Mixed precision enabled\")\n    except Exception as e:\n        print(\"âš ï¸ Mixed precision not enabled:\", e)\n\n# =========================\n# Helpers\n# =========================\ndef list_images(folder: Path):\n    exts = {\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\", \".tif\", \".tiff\"}\n    return [p for p in folder.rglob(\"*\") if p.is_file() and p.suffix.lower() in exts]\n\ndef save_list(path, arr):\n    with open(path, \"w\") as f:\n        for x in arr:\n            f.write(str(x) + \"\\n\")\n\n# =========================\n# Load files (with cap safety)\n# =========================\nassert DATASET_DIR.exists(), f\"Missing dataset dir: {DATASET_DIR}\"\n\nclasses = sorted([p.name for p in DATASET_DIR.iterdir() if p.is_dir()])\nprint(\"âœ… Classes found:\", classes)\n\nfiles, labels = [], []\nraw_counts = {}\n\nMAX_PER_CLASS = CONFIG[\"max_per_class\"]\n\nfor c in classes:\n    imgs = list_images(DATASET_DIR / c)\n    raw_counts[c] = len(imgs)\n\n    if MAX_PER_CLASS is not None and len(imgs) > MAX_PER_CLASS:\n        # deterministic shuffle (seeded)\n        rng = np.random.default_rng(CONFIG[\"seed\"])\n        idx = rng.permutation(len(imgs))[:MAX_PER_CLASS]\n        imgs = [imgs[i] for i in idx]\n\n    files.extend([str(p) for p in imgs])\n    labels.extend([c] * len(imgs))\n\nprint(\"\\nðŸ“Š Raw class counts (before cap):\")\nprint(raw_counts)\n\nfinal_counts = {c: labels.count(c) for c in classes}\nprint(\"\\nðŸ“Š Final class counts (after cap):\")\nprint(final_counts)\n\njson.dump(raw_counts, open(REPORTS_DIR / \"class_counts_raw.json\", \"w\"), indent=2)\njson.dump(final_counts, open(REPORTS_DIR / \"class_counts_used.json\", \"w\"), indent=2)\n\n# mapping\nclass_to_idx = {c:i for i,c in enumerate(classes)}\nidx_to_class = {i:c for c,i in class_to_idx.items()}\n\nmapping = {\"classes\": classes, \"class_to_idx\": class_to_idx}\njson.dump(mapping, open(MODEL_DIR / \"class_mapping.json\", \"w\"), indent=2)\n\nX = np.array(files)\ny = np.array([class_to_idx[c] for c in labels], dtype=np.int32)\n\n# =========================\n# Stratified split (70/15/15)\n# =========================\ntmp_split = CONFIG[\"val_split\"] + CONFIG[\"test_split\"]  # 0.30\nX_train, X_tmp, y_train, y_tmp = train_test_split(\n    X, y, test_size=tmp_split, stratify=y, random_state=CONFIG[\"seed\"]\n)\nX_val, X_test, y_val, y_test = train_test_split(\n    X_tmp, y_tmp, test_size=0.5, stratify=y_tmp, random_state=CONFIG[\"seed\"]\n)\n\nprint(\"\\nâœ… Split sizes:\", len(X_train), len(X_val), len(X_test))\n\nsave_list(SPLITS_DIR/\"train_files.txt\", X_train)\nsave_list(SPLITS_DIR/\"val_files.txt\", X_val)\nsave_list(SPLITS_DIR/\"test_files.txt\", X_test)\n\n# =========================\n# Make TEST.zip (class folders)\n# =========================\ntmp_test_root = OUT_DIR / \"test_folder_for_zip\"\nif tmp_test_root.exists():\n    shutil.rmtree(tmp_test_root)\nfor c in classes:\n    (tmp_test_root / c).mkdir(parents=True, exist_ok=True)\n\nfor fp, yi in zip(X_test, y_test):\n    c = idx_to_class[int(yi)]\n    shutil.copy2(fp, tmp_test_root / c / Path(fp).name)\n\nif TEST_ZIP.exists():\n    TEST_ZIP.unlink()\nwith zipfile.ZipFile(TEST_ZIP, \"w\", zipfile.ZIP_DEFLATED) as zf:\n    for p in tmp_test_root.rglob(\"*\"):\n        if p.is_file():\n            zf.write(p, arcname=str(p.relative_to(tmp_test_root)))\n\nprint(\"âœ… TEST.zip created:\", TEST_ZIP)\n\n# =========================\n# Data pipeline\n# =========================\nIMG_SIZE = (CONFIG[\"img_size\"], CONFIG[\"img_size\"])\n\naugment = tf.keras.Sequential([\n    tf.keras.layers.RandomFlip(\"horizontal\"),\n    tf.keras.layers.RandomRotation(0.15),\n    tf.keras.layers.RandomZoom(0.2),\n    tf.keras.layers.RandomContrast(0.2),\n], name=\"augment\")\n\ndef decode_resize(path):\n    img = tf.io.read_file(path)\n    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n    img = tf.image.resize(img, IMG_SIZE)\n    img = tf.cast(img, tf.float32)\n    return img\n\ndef make_ds(paths, labels, training=False):\n    paths = tf.constant(paths, dtype=tf.string)\n    labels = tf.constant(labels, dtype=tf.int32)\n    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n\n    if training:\n        ds = ds.shuffle(2048, seed=CONFIG[\"seed\"], reshuffle_each_iteration=True)\n\n    def _map(p, y):\n        img = decode_resize(p)\n        if training:\n            img = augment(img, training=True)\n        img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n        y_oh = tf.one_hot(y, depth=len(classes))\n        return img, y_oh\n\n    ds = ds.map(_map, num_parallel_calls=tf.data.AUTOTUNE)\n    ds = ds.batch(CONFIG[\"batch_size\"]).prefetch(tf.data.AUTOTUNE)\n    return ds\n\ntrain_ds = make_ds(X_train, y_train, training=True)\nval_ds = make_ds(X_val, y_val, training=False)\ntest_ds = make_ds(X_test, y_test, training=False)\n\n# =========================\n# Class weights\n# =========================\ncw = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\nclass_weights = {int(i): float(w) for i, w in zip(np.unique(y_train), cw)}\nprint(\"\\nâœ… Class weights:\", class_weights)\n\n# =========================\n# Build small model (MobileNetV2)\n# =========================\ninputs = tf.keras.Input(shape=(CONFIG[\"img_size\"], CONFIG[\"img_size\"], 3))\nbase = tf.keras.applications.MobileNetV2(include_top=False, weights=\"imagenet\", input_tensor=inputs)\nbase.trainable = False\n\nx = tf.keras.layers.GlobalAveragePooling2D()(base.output)\nx = tf.keras.layers.BatchNormalization()(x)\nx = tf.keras.layers.Dropout(CONFIG[\"dropout\"])(x)\n\n# keep output float32 for stability under mixed precision\noutputs = tf.keras.layers.Dense(len(classes), activation=\"softmax\", dtype=\"float32\")(x)\n\nmodel = tf.keras.Model(inputs, outputs)\nprint(\"\\nâœ… Model params:\", model.count_params())\n\ndef compile_model(lr):\n    opt = tf.keras.optimizers.Adam(learning_rate=lr, clipnorm=1.0)\n    loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=CONFIG[\"label_smoothing\"])\n    model.compile(optimizer=opt, loss=loss, metrics=[\"accuracy\"])\n\nckpt_path = MODEL_DIR / \"model_best.keras\"\ncallbacks = [\n    tf.keras.callbacks.ModelCheckpoint(ckpt_path, monitor=\"val_loss\", save_best_only=True, verbose=1),\n    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=8, restore_best_weights=True, verbose=1),\n    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=4, verbose=1),\n]\n\n# =========================\n# Train phase 1\n# =========================\nprint(\"\\nðŸŸ¦ Phase 1 (frozen base)...\")\ncompile_model(CONFIG[\"lr_phase1\"])\nh1 = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=CONFIG[\"epochs_phase1\"],\n    class_weight=class_weights,\n    callbacks=callbacks,\n    verbose=1\n)\n\n# =========================\n# Train phase 2\n# =========================\nprint(\"\\nðŸŸ¦ Phase 2 (fine-tune top %)...\")\nbase.trainable = True\nn_layers = len(base.layers)\nunfreeze_from = int(n_layers * (1 - CONFIG[\"unfreeze_top_percent\"]))\n\nfor i, layer in enumerate(base.layers):\n    layer.trainable = (i >= unfreeze_from)\n\ncompile_model(CONFIG[\"lr_phase2\"])\nh2 = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=CONFIG[\"epochs_phase2\"],\n    class_weight=class_weights,\n    callbacks=callbacks,\n    verbose=1\n)\n\n# =========================\n# Load best + evaluate\n# =========================\nbest_model = tf.keras.models.load_model(ckpt_path)\nprint(\"\\nâœ… Loaded best:\", ckpt_path)\n\nprobs = best_model.predict(test_ds, verbose=1)\ny_pred = np.argmax(probs, axis=1)\ny_true = y_test\n\nacc = float(np.mean(y_pred == y_true))\nprint(\"\\nâœ… Test accuracy:\", round(acc, 4))\n\ncm = confusion_matrix(y_true, y_pred)\nrep_text = classification_report(y_true, y_pred, target_names=classes, zero_division=0)\nrep_dict = classification_report(y_true, y_pred, target_names=classes, zero_division=0, output_dict=True)\n\njson.dump(cm.tolist(), open(REPORTS_DIR/\"confusion_matrix.json\",\"w\"), indent=2)\njson.dump(rep_dict, open(REPORTS_DIR/\"classification_report.json\",\"w\"), indent=2)\nprint(\"\\nClassification Report:\\n\", rep_text)\n\n# =========================\n# Plots\n# =========================\ndef merged_history(histories, key):\n    out = []\n    for h in histories:\n        out += h.history.get(key, [])\n    return out\n\ntrain_acc = merged_history([h1, h2], \"accuracy\")\nval_acc = merged_history([h1, h2], \"val_accuracy\")\ntrain_loss = merged_history([h1, h2], \"loss\")\nval_loss = merged_history([h1, h2], \"val_loss\")\n\nplt.figure()\nplt.plot(train_acc)\nplt.plot(val_acc)\nplt.xlabel(\"epoch\"); plt.ylabel(\"accuracy\")\nplt.legend([\"train\",\"val\"])\nplt.title(\"Accuracy\")\nplt.savefig(PLOTS_DIR/\"accuracy.png\", bbox_inches=\"tight\")\nplt.close()\n\nplt.figure()\nplt.plot(train_loss)\nplt.plot(val_loss)\nplt.xlabel(\"epoch\"); plt.ylabel(\"loss\")\nplt.legend([\"train\",\"val\"])\nplt.title(\"Loss\")\nplt.savefig(PLOTS_DIR/\"loss.png\", bbox_inches=\"tight\")\nplt.close()\n\nplt.figure(figsize=(8,6))\nplt.imshow(cm)\nplt.xticks(range(len(classes)), classes, rotation=45, ha=\"right\")\nplt.yticks(range(len(classes)), classes)\nplt.title(\"Confusion Matrix\")\nplt.colorbar()\nplt.tight_layout()\nplt.savefig(PLOTS_DIR/\"confusion_matrix.png\", bbox_inches=\"tight\")\nplt.close()\n\nprint(\"\\nâœ… Plots saved to:\", PLOTS_DIR)\n\n# =========================\n# Save formats\n# =========================\nbest_model.save(MODEL_DIR/\"model_best.h5\")               # optional legacy\nbest_model.export(str(MODEL_DIR/\"saved_model\"))         # SavedModel for serving/tflite\nprint(\"\\nâœ… Saved:\")\nprint(\" -\", MODEL_DIR/\"model_best.keras\")\nprint(\" -\", MODEL_DIR/\"model_best.h5\")\nprint(\" -\", MODEL_DIR/\"saved_model\")\nprint(\" -\", MODEL_DIR/\"class_mapping.json\")\nprint(\" -\", TEST_ZIP)\n\nprint(\"\\nðŸ“¦ Outputs in:\", OUT_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T19:08:28.828131Z","iopub.execute_input":"2026-02-03T19:08:28.828744Z","iopub.status.idle":"2026-02-03T19:29:39.342335Z","shell.execute_reply.started":"2026-02-03T19:08:28.828704Z","shell.execute_reply":"2026-02-03T19:29:39.341600Z"}},"outputs":[{"name":"stdout","text":"GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\nâœ… Mixed precision enabled\nâœ… Classes found: ['cardboard', 'clean_water', 'glass', 'marine_trash', 'metal', 'oil_spill', 'paper', 'plastic']\n\nðŸ“Š Raw class counts (before cap):\n{'cardboard': 1533, 'clean_water': 2400, 'glass': 1998, 'marine_trash': 3073, 'metal': 1200, 'oil_spill': 1200, 'paper': 1380, 'plastic': 1709}\n\nðŸ“Š Final class counts (after cap):\n{'cardboard': 1200, 'clean_water': 1200, 'glass': 1200, 'marine_trash': 1200, 'metal': 1200, 'oil_spill': 1200, 'paper': 1200, 'plastic': 1200}\n\nâœ… Split sizes: 6720 1440 1440\nâœ… TEST.zip created: /kaggle/working/run_final_train/TEST.zip\n\nâœ… Class weights: {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0, 7: 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/2281044373.py:213: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n  base = tf.keras.applications.MobileNetV2(include_top=False, weights=\"imagenet\", input_tensor=inputs)\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n\u001b[1m9406464/9406464\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n\nâœ… Model params: 2273352\n\nðŸŸ¦ Phase 1 (frozen base)...\nEpoch 1/10\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step - accuracy: 0.5158 - loss: 1.6153\nEpoch 1: val_loss improved from inf to 0.71316, saving model to /kaggle/working/run_final_train/models/model_best.keras\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 266ms/step - accuracy: 0.5165 - loss: 1.6133 - val_accuracy: 0.8271 - val_loss: 0.7132 - learning_rate: 0.0010\nEpoch 2/10\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.7838 - loss: 0.8784\nEpoch 2: val_loss improved from 0.71316 to 0.66513, saving model to /kaggle/working/run_final_train/models/model_best.keras\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 217ms/step - accuracy: 0.7838 - loss: 0.8783 - val_accuracy: 0.8590 - val_loss: 0.6651 - learning_rate: 0.0010\nEpoch 3/10\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.8031 - loss: 0.8194\nEpoch 3: val_loss improved from 0.66513 to 0.65454, saving model to /kaggle/working/run_final_train/models/model_best.keras\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 217ms/step - accuracy: 0.8031 - loss: 0.8193 - val_accuracy: 0.8694 - val_loss: 0.6545 - learning_rate: 0.0010\nEpoch 4/10\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.8195 - loss: 0.7616\nEpoch 4: val_loss improved from 0.65454 to 0.61973, saving model to /kaggle/working/run_final_train/models/model_best.keras\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 217ms/step - accuracy: 0.8195 - loss: 0.7616 - val_accuracy: 0.8813 - val_loss: 0.6197 - learning_rate: 0.0010\nEpoch 5/10\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 211ms/step - accuracy: 0.8293 - loss: 0.7454\nEpoch 5: val_loss improved from 0.61973 to 0.61195, saving model to /kaggle/working/run_final_train/models/model_best.keras\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 217ms/step - accuracy: 0.8294 - loss: 0.7454 - val_accuracy: 0.8861 - val_loss: 0.6119 - learning_rate: 0.0010\nEpoch 6/10\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.8465 - loss: 0.6967\nEpoch 6: val_loss improved from 0.61195 to 0.60835, saving model to /kaggle/working/run_final_train/models/model_best.keras\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 217ms/step - accuracy: 0.8465 - loss: 0.6967 - val_accuracy: 0.8847 - val_loss: 0.6083 - learning_rate: 0.0010\nEpoch 7/10\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.8362 - loss: 0.7119\nEpoch 7: val_loss did not improve from 0.60835\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 214ms/step - accuracy: 0.8362 - loss: 0.7119 - val_accuracy: 0.8868 - val_loss: 0.6165 - learning_rate: 0.0010\nEpoch 8/10\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.8336 - loss: 0.7191\nEpoch 8: val_loss did not improve from 0.60835\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 214ms/step - accuracy: 0.8336 - loss: 0.7191 - val_accuracy: 0.8910 - val_loss: 0.6094 - learning_rate: 0.0010\nEpoch 9/10\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 210ms/step - accuracy: 0.8400 - loss: 0.7121\nEpoch 9: val_loss improved from 0.60835 to 0.60569, saving model to /kaggle/working/run_final_train/models/model_best.keras\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 216ms/step - accuracy: 0.8400 - loss: 0.7120 - val_accuracy: 0.8882 - val_loss: 0.6057 - learning_rate: 0.0010\nEpoch 10/10\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.8441 - loss: 0.6921\nEpoch 10: val_loss improved from 0.60569 to 0.59470, saving model to /kaggle/working/run_final_train/models/model_best.keras\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 216ms/step - accuracy: 0.8441 - loss: 0.6921 - val_accuracy: 0.8854 - val_loss: 0.5947 - learning_rate: 0.0010\nRestoring model weights from the end of the best epoch: 10.\n\nðŸŸ¦ Phase 2 (fine-tune top %)...\nEpoch 1/15\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 207ms/step - accuracy: 0.7702 - loss: 0.8621\nEpoch 1: val_loss did not improve from 0.59470\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 268ms/step - accuracy: 0.7703 - loss: 0.8619 - val_accuracy: 0.8708 - val_loss: 0.6406 - learning_rate: 1.0000e-05\nEpoch 2/15\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.8199 - loss: 0.7552\nEpoch 2: val_loss did not improve from 0.59470\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 220ms/step - accuracy: 0.8199 - loss: 0.7552 - val_accuracy: 0.8813 - val_loss: 0.6389 - learning_rate: 1.0000e-05\nEpoch 3/15\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8190 - loss: 0.7493\nEpoch 3: val_loss did not improve from 0.59470\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 218ms/step - accuracy: 0.8191 - loss: 0.7492 - val_accuracy: 0.8903 - val_loss: 0.6163 - learning_rate: 1.0000e-05\nEpoch 4/15\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8392 - loss: 0.7117\nEpoch 4: val_loss did not improve from 0.59470\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 218ms/step - accuracy: 0.8392 - loss: 0.7116 - val_accuracy: 0.8951 - val_loss: 0.6012 - learning_rate: 1.0000e-05\nEpoch 5/15\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8602 - loss: 0.6725\nEpoch 5: val_loss improved from 0.59470 to 0.58674, saving model to /kaggle/working/run_final_train/models/model_best.keras\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 221ms/step - accuracy: 0.8602 - loss: 0.6724 - val_accuracy: 0.9014 - val_loss: 0.5867 - learning_rate: 1.0000e-05\nEpoch 6/15\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - accuracy: 0.8606 - loss: 0.6583\nEpoch 6: val_loss improved from 0.58674 to 0.58594, saving model to /kaggle/working/run_final_train/models/model_best.keras\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 221ms/step - accuracy: 0.8606 - loss: 0.6583 - val_accuracy: 0.9000 - val_loss: 0.5859 - learning_rate: 1.0000e-05\nEpoch 7/15\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8592 - loss: 0.6642\nEpoch 7: val_loss improved from 0.58594 to 0.58343, saving model to /kaggle/working/run_final_train/models/model_best.keras\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 221ms/step - accuracy: 0.8592 - loss: 0.6642 - val_accuracy: 0.9069 - val_loss: 0.5834 - learning_rate: 1.0000e-05\nEpoch 8/15\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8816 - loss: 0.6168\nEpoch 8: val_loss improved from 0.58343 to 0.57017, saving model to /kaggle/working/run_final_train/models/model_best.keras\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 222ms/step - accuracy: 0.8815 - loss: 0.6168 - val_accuracy: 0.9132 - val_loss: 0.5702 - learning_rate: 1.0000e-05\nEpoch 9/15\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8757 - loss: 0.6174\nEpoch 9: val_loss improved from 0.57017 to 0.56648, saving model to /kaggle/working/run_final_train/models/model_best.keras\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 222ms/step - accuracy: 0.8757 - loss: 0.6174 - val_accuracy: 0.9125 - val_loss: 0.5665 - learning_rate: 1.0000e-05\nEpoch 10/15\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8911 - loss: 0.5810\nEpoch 10: val_loss did not improve from 0.56648\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 218ms/step - accuracy: 0.8911 - loss: 0.5810 - val_accuracy: 0.9035 - val_loss: 0.5703 - learning_rate: 1.0000e-05\nEpoch 11/15\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - accuracy: 0.8962 - loss: 0.5779\nEpoch 11: val_loss did not improve from 0.56648\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 218ms/step - accuracy: 0.8962 - loss: 0.5780 - val_accuracy: 0.9035 - val_loss: 0.5731 - learning_rate: 1.0000e-05\nEpoch 12/15\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.8994 - loss: 0.5685\nEpoch 12: val_loss did not improve from 0.56648\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 218ms/step - accuracy: 0.8993 - loss: 0.5685 - val_accuracy: 0.9069 - val_loss: 0.5703 - learning_rate: 1.0000e-05\nEpoch 13/15\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 215ms/step - accuracy: 0.9001 - loss: 0.5629\nEpoch 13: val_loss did not improve from 0.56648\n\nEpoch 13: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-06.\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 219ms/step - accuracy: 0.9001 - loss: 0.5628 - val_accuracy: 0.9069 - val_loss: 0.5686 - learning_rate: 1.0000e-05\nEpoch 14/15\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.9049 - loss: 0.5613\nEpoch 14: val_loss improved from 0.56648 to 0.56168, saving model to /kaggle/working/run_final_train/models/model_best.keras\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 221ms/step - accuracy: 0.9049 - loss: 0.5613 - val_accuracy: 0.9090 - val_loss: 0.5617 - learning_rate: 5.0000e-06\nEpoch 15/15\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 214ms/step - accuracy: 0.9139 - loss: 0.5435\nEpoch 15: val_loss improved from 0.56168 to 0.55423, saving model to /kaggle/working/run_final_train/models/model_best.keras\n\u001b[1m210/210\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 221ms/step - accuracy: 0.9139 - loss: 0.5435 - val_accuracy: 0.9146 - val_loss: 0.5542 - learning_rate: 5.0000e-06\nRestoring model weights from the end of the best epoch: 15.\n\nâœ… Loaded best: /kaggle/working/run_final_train/models/model_best.keras\n\u001b[1m45/45\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step\n\nâœ… Test accuracy: 0.916\n\nClassification Report:\n               precision    recall  f1-score   support\n\n   cardboard       0.89      0.91      0.90       180\n clean_water       0.98      0.94      0.96       180\n       glass       0.88      0.91      0.89       180\nmarine_trash       0.99      1.00      0.99       180\n       metal       0.86      0.93      0.90       180\n   oil_spill       0.94      0.98      0.96       180\n       paper       0.89      0.87      0.88       180\n     plastic       0.92      0.79      0.85       180\n\n    accuracy                           0.92      1440\n   macro avg       0.92      0.92      0.92      1440\nweighted avg       0.92      0.92      0.92      1440\n\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"\nâœ… Plots saved to: /kaggle/working/run_final_train/plots\nINFO:tensorflow:Assets written to: /kaggle/working/run_final_train/models/saved_model/assets\n","output_type":"stream"},{"name":"stderr","text":"INFO:tensorflow:Assets written to: /kaggle/working/run_final_train/models/saved_model/assets\n","output_type":"stream"},{"name":"stdout","text":"Saved artifact at '/kaggle/working/run_final_train/models/saved_model'. The following endpoints are available:\n\n* Endpoint 'serve'\n  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_layer_10')]\nOutput Type:\n  TensorSpec(shape=(None, 8), dtype=tf.float32, name=None)\nCaptures:\n  132417294254160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417268559568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417268558992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417294254544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417294258960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417268563024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417268559184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417268559376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420779100304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132420779114320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417268555152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417286010448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417286010256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417268560336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417268558416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417286010832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417286000080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417286011216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417286009488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417286011792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417268559760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417286011600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417286011024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417286011984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417286012560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417286010640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417286005264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417286012368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417286012176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417286011408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417286006800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604748688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604748880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417286012752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604747728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604747344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604748496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604748304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604748112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604749840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604749264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604749456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604749648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604747536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604750800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604750224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604750416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604750608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604747920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604751760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604751184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604751376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604751568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604749072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604752720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604752144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604752336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604752528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604750032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604753680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604753104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604753296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604753488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604750992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604754640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604754064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604754256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604754448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604751952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604755600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604755024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604755216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604755408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604752912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604756560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604755984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604756176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604756368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604753872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604757520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604756944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604757136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604757328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604754832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604758480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604757904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604758096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604758288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604755792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604759440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604758864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604759056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604759248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604756752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604760400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604759824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604760016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604760208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604757712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604761360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604760784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604760976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604761168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604758672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604762320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604761744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604761936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604762128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604759632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604763280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604762704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604761552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604763088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604762896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604762512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604760592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610581392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610581584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417604763472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610580432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610580048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610581200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610581008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610580816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610582544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610581968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610582160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610582352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610580240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610583504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610582928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610583120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610583312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610580624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610584464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610583888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610584080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610584272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610581776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610585424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610584848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610585040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610585232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610582736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610586384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610585808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610586000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610586192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610583696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610587344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610586768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610586960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610587152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610584656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610588304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610587728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610587920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610588112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610585616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610589264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610588688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610588880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610589072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610586576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610590224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610589648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610589840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610590032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610587536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610591184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610590608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610590800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610590992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610588496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610592144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610591568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610591760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610591952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610589456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610593104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610592528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610592720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610592912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610590416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610594064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610593488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610593680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610593872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610591376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610595024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610594448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610594640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610594832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610592336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610595984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610595408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610594256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610595792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610595600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610595216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610593296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609663888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609664080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417610596176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609662928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609662544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609663696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609663504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609663312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609665040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609664464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609664656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609664848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609662736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609666000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609665424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609665616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609665808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609663120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609666960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609666384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609666576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609666768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609664272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609667920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609667344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609667536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609667728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609665232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609668880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609668304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609668496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609668688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609666192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609669840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609669264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609669456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609669648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609667152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609670800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609670224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609670416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609670608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609668112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609671760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609671184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609671376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609671568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609669072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609672720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609672144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609672336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609672528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609670032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609673680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609673104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609673296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609673488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609670992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609674640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609675216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609675408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609674832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609675024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609675792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n  132417609675600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n\nâœ… Saved:\n - /kaggle/working/run_final_train/models/model_best.keras\n - /kaggle/working/run_final_train/models/model_best.h5\n - /kaggle/working/run_final_train/models/saved_model\n - /kaggle/working/run_final_train/models/class_mapping.json\n - /kaggle/working/run_final_train/TEST.zip\n\nðŸ“¦ Outputs in: /kaggle/working/run_final_train\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"import shutil, zipfile\nfrom pathlib import Path\n\nRUN_DIR = Path(\"/kaggle/working/run_final_train\")\n\nMODEL_DIR   = RUN_DIR / \"models\"\nPLOTS_DIR   = RUN_DIR / \"plots\"\nREPORTS_DIR = RUN_DIR / \"reports\"\nTEST_ZIP    = RUN_DIR / \"TEST.zip\"\n\nBEST_KERAS  = MODEL_DIR / \"model_best.keras\"\nCLASS_MAP   = MODEL_DIR / \"class_mapping.json\"\nSAVED_MODEL = MODEL_DIR / \"saved_model\"\n\nassert BEST_KERAS.exists(), f\"Missing model: {BEST_KERAS}\"\nassert CLASS_MAP.exists(), f\"Missing mapping: {CLASS_MAP}\"\n\n# Output bundle folder + zip path\nBUNDLE_DIR = Path(\"/kaggle/working/deploy_bundle\")\nDEPLOY_ZIP = Path(\"/kaggle/working/deploy_bundle.zip\")\n\n# Reset bundle folder\nif BUNDLE_DIR.exists():\n    shutil.rmtree(BUNDLE_DIR, ignore_errors=True)\nBUNDLE_DIR.mkdir(parents=True, exist_ok=True)\n\n# 1) Copy model files\n(BUNDLE_DIR / \"models\").mkdir(exist_ok=True)\nshutil.copy2(BEST_KERAS, BUNDLE_DIR / \"models\" / BEST_KERAS.name)\nshutil.copy2(CLASS_MAP,  BUNDLE_DIR / \"models\" / CLASS_MAP.name)\n\n# Copy saved_model if present\nif SAVED_MODEL.exists() and SAVED_MODEL.is_dir():\n    shutil.copytree(SAVED_MODEL, BUNDLE_DIR / \"models\" / \"saved_model\", dirs_exist_ok=True)\n\n# 2) Copy optional artifacts\nif REPORTS_DIR.exists() and REPORTS_DIR.is_dir():\n    shutil.copytree(REPORTS_DIR, BUNDLE_DIR / \"reports\", dirs_exist_ok=True)\n\nif PLOTS_DIR.exists() and PLOTS_DIR.is_dir():\n    shutil.copytree(PLOTS_DIR, BUNDLE_DIR / \"plots\", dirs_exist_ok=True)\n\nif TEST_ZIP.exists():\n    shutil.copy2(TEST_ZIP, BUNDLE_DIR / TEST_ZIP.name)\n\n# 3) Add README\n(BUNDLE_DIR / \"README.txt\").write_text(\n\"\"\"Deploy bundle contents:\n\nmodels/\n  - model_best.keras\n  - class_mapping.json\n  - saved_model/ (optional)\n\nreports/ (optional)\nplots/   (optional)\nTEST.zip (optional)\n\nBackend rules suggestion:\n- If predicted == clean_water -> no pollution detected\n- Optional: if max_prob < 0.60 -> unknown\n\"\"\"\n)\n\n# 4) Zip it\nif DEPLOY_ZIP.exists():\n    DEPLOY_ZIP.unlink()\n\nwith zipfile.ZipFile(DEPLOY_ZIP, \"w\", zipfile.ZIP_DEFLATED) as zf:\n    for fp in BUNDLE_DIR.rglob(\"*\"):\n        if fp.is_file():\n            zf.write(fp, arcname=str(fp.relative_to(BUNDLE_DIR)))\n\nprint(\"âœ… Bundle folder:\", BUNDLE_DIR)\nprint(\"ðŸ“¦ Zip created:\", DEPLOY_ZIP)\nprint(\"Size (MB):\", round(DEPLOY_ZIP.stat().st_size / (1024*1024), 2))\nprint(\"âž¡ï¸ Download from Kaggle output: /kaggle/working/deploy_bundle.zip\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T19:42:53.689206Z","iopub.execute_input":"2026-02-03T19:42:53.689967Z","iopub.status.idle":"2026-02-03T19:42:57.193350Z","shell.execute_reply.started":"2026-02-03T19:42:53.689935Z","shell.execute_reply":"2026-02-03T19:42:57.192653Z"}},"outputs":[{"name":"stdout","text":"âœ… Bundle folder: /kaggle/working/deploy_bundle\nðŸ“¦ Zip created: /kaggle/working/deploy_bundle.zip\nSize (MB): 65.47\nâž¡ï¸ Download from Kaggle output: /kaggle/working/deploy_bundle.zip\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"import os, zipfile\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nDATA_DIR = Path(\"/kaggle/working/run_min_balance/balanced_dataset\")\nOUT_ZIP = Path(\"/kaggle/working/balanced_dataset.zip\")\n\nassert DATA_DIR.exists(), f\"Dataset folder not found: {DATA_DIR}\"\n\ndef list_images(folder: Path):\n    exts = {\".jpg\", \".jpeg\", \".png\", \".webp\", \".bmp\", \".tif\", \".tiff\"}\n    return [p for p in folder.rglob(\"*\") if p.is_file() and p.suffix.lower() in exts]\n\n# 1) Print counts\nclasses = sorted([p for p in DATA_DIR.iterdir() if p.is_dir()])\nprint(\"âœ… Classes:\", [c.name for c in classes])\n\ntotal = 0\nprint(\"\\nðŸ“Š Images per class:\")\nfor c in classes:\n    n = len(list_images(c))\n    total += n\n    print(f\" - {c.name:15s}: {n}\")\n\nprint(\"\\nâœ… Total images:\", total)\n\n# 2) Zip it\nif OUT_ZIP.exists():\n    OUT_ZIP.unlink()\n\nall_files = [p for p in DATA_DIR.rglob(\"*\") if p.is_file()]\nprint(\"\\nðŸŸ¦ Zipping... files:\", len(all_files))\n\nwith zipfile.ZipFile(OUT_ZIP, \"w\", zipfile.ZIP_DEFLATED) as zf:\n    for fp in tqdm(all_files):\n        zf.write(fp, arcname=str(fp.relative_to(DATA_DIR)))\n\nprint(\"\\nðŸ“¦ Done!\")\nprint(\"Zip path:\", OUT_ZIP)\nprint(\"Size (GB):\", round(OUT_ZIP.stat().st_size / (1024**3), 3))\nprint(\"âž¡ï¸ Download from Kaggle output: /kaggle/working/balanced_dataset.zip\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-03T19:50:46.791682Z","iopub.execute_input":"2026-02-03T19:50:46.792031Z","iopub.status.idle":"2026-02-03T19:50:53.683217Z","shell.execute_reply.started":"2026-02-03T19:50:46.792005Z","shell.execute_reply":"2026-02-03T19:50:53.682504Z"}},"outputs":[{"name":"stdout","text":"âœ… Classes: ['cardboard', 'clean_water', 'glass', 'marine_trash', 'metal', 'oil_spill', 'paper', 'plastic']\n\nðŸ“Š Images per class:\n - cardboard      : 1533\n - clean_water    : 2400\n - glass          : 1998\n - marine_trash   : 3073\n - metal          : 1200\n - oil_spill      : 1200\n - paper          : 1380\n - plastic        : 1709\n\nâœ… Total images: 14493\n\nðŸŸ¦ Zipping... files: 14493\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14493/14493 [00:06<00:00, 2195.83it/s]","output_type":"stream"},{"name":"stdout","text":"\nðŸ“¦ Done!\nZip path: /kaggle/working/balanced_dataset.zip\nSize (GB): 0.155\nâž¡ï¸ Download from Kaggle output: /kaggle/working/balanced_dataset.zip\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}